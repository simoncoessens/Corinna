# Additional Obligations for Online Platforms
# DSA Articles 20-28 (Section 3 of Chapter III)
# These apply to: Online Platforms, Marketplaces, VLOPs
# (In addition to Intermediary + Hosting obligations)
#
# NOTE: Articles 19 and 29 provide SME exemptions for platforms with
# <50 employees AND <€10M turnover (unless VLOP/marketplace)

category: Online Platform
description: |
  Online platforms store AND disseminate content to the public. These obligations
  address the unique risks of public-facing platforms including viral spread of
  harmful content, dark patterns, and advertising transparency.

sme_exemption_note: |
  Small/micro enterprises (< 50 employees, < €10M turnover) are EXEMPT from 
  Section 3 obligations UNLESS they are marketplaces or VLOPs.

obligations:
  - article: 20
    title: Internal complaint-handling system
    context: |
      Beyond just explaining decisions (Art. 17), platforms must provide a real
      complaint mechanism where users can challenge content moderation decisions.
      Human review must be available upon request.
    key_requirements:
      - Free, electronic, easy-to-use complaint system
      - Allow complaints against any content moderation decision
      - Handle complaints in timely, non-discriminatory manner
      - Provide human review (not purely automated)
      - Inform users of outcome and redress options

  - article: 21
    title: Out-of-court dispute settlement
    context: |
      Users must be able to escalate disputes to certified independent bodies.
      The platform must engage in good faith with these bodies.
    key_requirements:
      - Enable users to select certified dispute settlement body
      - Engage in good faith with selected body
      - Accept binding decisions from settlement body
      - Bear reasonable costs for users who prevail

  - article: 22
    title: Trusted flaggers
    context: |
      Certain expert entities (consumer orgs, anti-hate groups, etc.) can be 
      certified as "trusted flaggers." Their notices get priority processing.
    key_requirements:
      - Process trusted flagger notices with priority
      - Ensure timely decisions on trusted flagger notices
      - Maintain respectful cooperation with trusted flaggers

  - article: 23
    title: Measures and protection against misuse
    context: |
      You must take action against users who frequently submit manifestly 
      unfounded notices or complaints (abusers of the system).
    key_requirements:
      - Suspend processing for repeat abusers
      - Establish clear policy on misuse
      - Issue warnings before suspension
      - Maintain proportionality in enforcement

  - article: 24
    title: Transparency reporting (enhanced for platforms)
    context: |
      Platforms have MORE extensive reporting requirements than basic 
      intermediaries. Reports must include detailed moderation statistics.
    key_requirements:
      - All Art. 15 requirements PLUS additional data
      - Report on complaint-handling system usage
      - Report on out-of-court dispute outcomes
      - Include automated moderation statistics
      - Publish every 6 months (not annually)

  - article: 25
    title: Online interface design and organisation
    context: |
      DARK PATTERNS ARE BANNED. Your interface must not deceive, manipulate, 
      or impair users' ability to make free decisions. No "confirm-shaming," 
      hidden unsubscribe buttons, or similar tricks.
    key_requirements:
      - No deceptive interface design
      - No manipulation to influence decisions
      - Clear presentation of choices
      - No making certain choices harder than others
      - Advertising must be clearly identifiable

  - article: 26
    title: Advertising on online platforms
    context: |
      Every ad must be clearly labeled AS an ad. Users must know WHO paid for 
      it and the main targeting parameters. This enables informed decisions 
      and accountability.
    key_requirements:
      - Clearly mark content as advertising
      - Identify advertiser (or who paid)
      - Provide meaningful targeting information
      - Real-time, visible, and accessible information

  - article: 27
    title: Recommender system transparency
    context: |
      If you use algorithms to recommend/rank content, users must understand 
      the main parameters used. You must offer at least one option NOT based 
      on user profiling.
    key_requirements:
      - Explain main parameters of recommender systems
      - Provide this in ToS in accessible manner
      - Offer non-profiled recommendation option
      - Allow users to modify recommendation parameters

  - article: 28
    title: Online protection of minors
    context: |
      Platforms accessible to minors must implement age-appropriate protections.
      Content moderation must consider child safety. No profiling-based 
      advertising to minors.
    key_requirements:
      - Design services with minor protection in mind
      - Implement appropriate safeguards
      - No targeted advertising based on profiling to known minors
      - High level of privacy and safety for minors
