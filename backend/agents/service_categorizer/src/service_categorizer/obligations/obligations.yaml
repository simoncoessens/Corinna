# Consolidated DSA Obligations
# This file contains all obligations with mappings to service categories and size-based exemptions

# Service Categories:
# - Mere Conduit
# - Caching
# - Hosting
# - Online Platform
# - Online Marketplace
# - Search Engine

# Size Categories:
# - Very Large Platform (VLOP/VLOSE): ≥45M EU users
# - SME Exemption Eligible: <50 employees AND <€10M turnover

# =============================================================================
# Selection rules (authoritative)
#
# Logic:
# - Start with the base list for the service category (category_articles[category])
# - If VLOP/VLOSE: add extra articles (size_rules.vlop_vlose.extra_articles)
# - If SME exemption eligible: remove exemptable articles (size_rules.sme_exemption_eligible.exempt_articles)
#   but ONLY for Online Platforms that are NOT marketplaces and NOT VLOPs/VLOSEs.
# =============================================================================

category_articles:
  Mere Conduit: [11, 12, 13, 14, 15]
  Caching: [11, 12, 13, 14, 15]
  Hosting: [11, 12, 13, 14, 15, 16, 17, 18]
  Online Platform:
    [11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, "24.3", 25, 26, 27, 28]
  Online Marketplace:
    [
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      20,
      21,
      22,
      23,
      24,
      "24.3",
      25,
      26,
      27,
      28,
      30,
      31,
      32,
    ]
  Search Engine: [9, 10, 11, 12, 13, 14, 15]

size_rules:
  # Size rules modify the *base* `category_articles` selection.
  #
  # How to use:
  # - **VLOP/VLOSE**: if `is_vlop_vlose == true`, we ADD `extra_articles` to the base list
  # - **SME exemption**: if `qualifies_for_sme_exemption == true`, we REMOVE `exempt_articles`
  #   from the base list (but only when the rule applies and the guards allow it).
  #
  # Important: SME exemptions differ by category:
  # - Online Platforms: SME-eligible providers can be exempt from Section 3 (Art. 20–28)
  # - Online Marketplaces: SME exemption does remove obligation in Section 3 (Art. 20–28), but in addition, they don't have the obligations of Section 4 (Article 30-32).
  #
  # Notes:
  # - We never infer category membership here; we only modify by article numbers.
  # - If you add a new obligation entry below, also update the relevant article lists here.
  vlop_vlose:
    # Only apply this rule when the *service category* is one of these.
    applicable_categories:
      ["Online Platform", "Online Marketplace", "Search Engine"]
    # Extra obligations that apply *in addition to* the base category when VLOP/VLOSE is true.
    extra_articles: [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]
  sme_exemption_online_platform:
    # SME exemption for Section 3 obligations (Art. 20–28) for *non-marketplace* online platforms DO NOT APPLY if the company is designated as VLOP/VLOSE.
    # SME exemption for Section 4 obligations (Art. 30–32) for marketplace online platforms DO NOT APPLY if the company is designated as VLOP/VLOSE.

    # Note: Article 24.3 is NOT exempt (must be excluded from exempt_articles).
    applicable_categories: ["Online Platform"]
    exempt_articles: [20, 21, 22, 23, 24, 25, 26, 27, 28]
    # Articles that are NOT exempt even when SME-eligible (e.g., "24.3")
    not_exempt_articles: ["24.3"]
    # Guard: if true, do NOT apply SME exemption removals when the service is a marketplace.
    not_if_marketplace: true
    # Guard: if true, do NOT apply SME exemption removals when the service is VLOP/VLOSE.
    not_if_vlop_vlose: true
  sme_exemption_online_marketplace:
    # Marketplaces benefit from the SME exemption for Section 3 (Art. 19(2)).
    # Keep this rule explicit so the distinction is visible in config; it removes nothing.
    applicable_categories: ["Online Marketplace"]
    exempt_articles: []
    not_if_marketplace: false
    not_if_vlop_vlose: true

obligations:
  # ============================================================================
  # Section 1: Intermediary Service Obligations (Articles 11-15)
  # Apply to: ALL intermediary services (Mere Conduit, Caching, Hosting,
  #           Online Platform, Online Marketplace, Search Engine)
  # ============================================================================

  - article: 11
    title: Points of contact for Member States' authorities
    context: |
      Recital 42  - In order to facilitate smooth and efficient two-way communications, including, where relevant, by acknowledging the receipt of such communications, relating to matters covered by this Regulation, providers of intermediary services should be required to designate a single electronic point of contact and to publish and update relevant information relating to that point of contact, including the languages to be used in such communications. The electronic point of contact can also be used by trusted flaggers and by professional entities which are under a specific relationship with the provider of intermediary services. In contrast to the legal representative, the electronic point of contact should serve operational purposes and should not be required to have a physical location. Providers of intermediary services can designate the same single point of contact for the requirements of this Regulation as well as for the purposes of other acts of Union law. When specifying the languages of communication, providers of intermediary services are encouraged to ensure that the languages chosen do not in themselves constitute an obstacle to communication. Where necessary, it should be possible for providers of intermediary services and Member States authorities to reach a separate agreement on the language of communication, or to seek alternative means to overcome the language barrier, including by using all available technological means or internal and external human resources.
    key_requirements:
      - Designate electronic point of contact
      - Publish contact information publicly
      - Ensure responsiveness to authority requests

  - article: 12
    title: Points of contact for recipients of the service
    context: |
      Recital 43 - Providers of intermediary services should also be required to designate a single point of contact for recipients of services, enabling rapid, direct and efficient communication in particular by easily accessible means such as telephone numbers, email addresses, electronic contact forms, chatbots or instant messaging. It should be explicitly indicated when a recipient of the service communicates with chatbots. Providers of intermediary services should allow recipients of services to choose means of direct and efficient communication which do not solely rely on automated tools. Providers of intermediary services should make all reasonable efforts to guarantee that sufficient human and financial resources are allocated to ensure that this communication is performed in a timely and efficient manner.
    key_requirements:
      - Provide user-accessible contact mechanism
      - Ensure efficient response capability
      - Make contact information easily accessible

  - article: 13
    title: Legal representatives
    context: |
      Recital 44 - Providers of intermediary services that are established in a third country and that offer services in the Union should designate a sufficiently mandated legal representative in the Union and provide information relating to their legal representatives to the relevant authorities and make it publicly available. In order to comply with that obligation, such providers of intermediary services should ensure that the designated legal representative has the necessary powers and resources to cooperate with the relevant authorities. This could be the case, for example, where a provider of intermediary services appoints a subsidiary undertaking of the same group as the provider, or its parent undertaking, if that subsidiary or parent undertaking is established in the Union. However, it might not be the case, for instance, when the legal representative is subject to reconstruction proceedings, bankruptcy, or personal or corporate insolvency. That obligation should allow for the effective oversight and, where necessary, enforcement of this Regulation in relation to those providers. It should be possible for a legal representative to be mandated, in accordance with national law, by more than one provider of intermediary services. It should be possible for the legal representative to also function as a point of contact, provided the relevant requirements of this Regulation are complied with.
    key_requirements:
      - Designate legal representative in an EU Member State
      - Grant representative authority to be addressed by authorities
      - Publish representative's contact details

  - article: 14
    title: Terms of service
    context: |
      Recital 45 - In view of their central role in intermediating access to information, services and economic activities online and in public debate, it is necessary to ensure that users of intermediary services have adequate information about restrictions on the use of such services. In that context, where applicable in a given case, such information should also include any general use restrictions in respect of material protected under Union and national law in areas such as intellectual property rights. Therefore, it is necessary to impose certain information obligations regarding those aspects, and in particular regarding any policies and procedures deployed in connection with content moderation. These transparency requirements should apply to all providers of intermediary services, including in situations where a provider is established or has users outside the Union. The obligations should complement those imposed by Directive 2000/31/EC and national or Union legislation laying down requirements for providers of intermediary services to include certain information on their online interface or otherwise to make it available to users. Union legislation regarding consumer protection and product safety already provides for certain information obligations in that regard for providers of intermediary services. In order to ensure user-friendly communication of any restrictions related to the use of a given intermediary service, as well as any policies, procedures, measures and tools used for the purpose of content moderation, including in respect of decisions taken at the request of public authorities, of trusted flaggers or of other third parties, the terms and conditions should be written in clear and unambiguous language, and should be available in an easily accessible and machine-readable format."
      Recital 46 - It is important that content moderation activities, both by users reporting allegedly illegal content or other information subject to restrictions and by providers of intermediary services reviewing, restricting and removing such content or otherwise restricting access to it, are diligent, objective and proportionate, with regard not only to the issue of whether the content is illegal or not, but also having due regard to the rights and legitimate interests of all parties, in particular their fundamental rights under the Charter. In this regard, particular attention should be paid to the freedom of expression, freedom and pluralism of the media, and to other fundamental rights and freedoms as enshrined in the Charter.
    key_requirements:
      - Include content moderation policies in ToS
      - Explain restrictions on content and user behavior
      - Describe algorithmic decision-making if used
      - Use clear, plain language
      - Make ToS publicly available

  - article: 15
    title: Transparency reporting obligations
    context: |
      Recital 47 - Providers of intermediary services should regularly make information available to the public on the content moderation that they carried out at the request of authorities or their own initiative. Such information should include transparent, user-friendly, sufficiently detailed information on the content moderation carried out in respect of specific items of illegal or incompatible content in response to notices or based on own-initiative investigations, such as information on the scale of removals, disabling access, the visibility of content, on suspensions or termination of services to recipients, and suspension or termination of accounts of recipients on the basis of complaints by recipients and own-initiative investigations. The information should also include information on the functioning of any complaint-handling systems and on out-of-court dispute settlement. Without prejudice to the supervision and enforcement pursuant to this Regulation, such information should further include transparency on orders received from Member States' authorities and from Union institutions, bodies, offices and agencies. Providers should be able to comply with those transparency obligations based on their internal procedures regarding restrictions imposed on content or accounts that contravene their terms and conditions, by grouping the relevant data per type of restriction. The application of this obligation should not impose a disproportionate burden on providers that are micro or small enterprises, unless those providers have been designated as very large online platforms.
      Recital 49 – General Transparency Reporting Framework:"To ensure an adequate level of transparency and accountability, providers of intermediary services should make publicly available an annual report in a machine-readable format, in accordance with the harmonised requirements contained in this Regulation, on the content moderation in which they engage, including the measures taken as a result of the application and enforcement of their terms and conditions. However, in order to avoid disproportionate burdens, those transparency reporting obligations should not apply to providers that are micro or small enterprises as defined in Commission Recommendation 2003/361/EC and which are not very large online platforms within the meaning of this Regulation.
      Recital 50 – Providers of hosting services play a particularly important role in tackling illegal content online, as they store information provided by and at the request of the recipients of the service and typically give other recipients access thereto, sometimes on a large scale. It is important that all providers of hosting services, regardless of their size, put in place easily accessible and user-friendly notice and action mechanisms that facilitate the notification of specific items of information that the notifying party considers to be illegal content to the provider of hosting services concerned ('notice'), pursuant to which that provider can decide whether or not it agrees with that assessment and wishes to remove or disable access to that content ('action'). Such mechanisms should be clearly identifiable, located close to the information in question and at least as easy to find and use as notification mechanisms for content that violates the terms and conditions of the hosting service provider. Provided the requirements on notices are met, it should be possible for individuals or entities to notify multiple specific items of allegedly illegal content through a single notice in order to ensure the effective operation of notice and action mechanisms. The notification mechanism should allow, but not require, the identification of the individual or the entity submitting a notice. For some types of items of information notified, the identity of the individual or the entity submitting a notice might be necessary to determine whether the information in question constitutes illegal content, as alleged. The obligation to put in place notice and action mechanisms should apply, for instance, to file storage and sharing services, web hosting services, advertising servers and paste bins, in so far as they qualify as hosting services covered by this Regulation.
    key_requirements:
      - Publish annual transparency report
      - Include statistics on orders from authorities
      - Report on notice-and-action activities
      - Detail content moderation measures
      - Make report publicly accessible

  # ============================================================================
  # Section 2: Hosting Service Obligations (Articles 16-18)
  # Apply to: Hosting, Online Platform, Online Marketplace, Search Engine
  # ============================================================================

  - article: 16
    title: Notice and action mechanisms
    context: |
      Recital 50 - Providers of hosting services play a particularly important role in tackling illegal content online, as they store information provided by and at the request of the recipients of the service and typically give other recipients access thereto, sometimes on a large scale. It is important that all providers of hosting services, regardless of their size, put in place easily accessible and user-friendly notice and action mechanisms that facilitate the notification of specific items of information that the notifying party considers to be illegal content to the provider of hosting services concerned ('notice'), pursuant to which that provider can decide whether or not it agrees with that assessment and wishes to remove or disable access to that content ('action'). Such mechanisms should be clearly identifiable, located close to the information in question and at least as easy to find and use as notification mechanisms for content that violates the terms and conditions of the hosting service provider. Provided the requirements on notices are met, it should be possible for individuals or entities to notify multiple specific items of allegedly illegal content through a single notice in order to ensure the effective operation of notice and action mechanisms. The notification mechanism should allow, but not require, the identification of the individual or the entity submitting a notice. For some types of items of information notified, the identity of the individual or the entity submitting a notice might be necessary to determine whether the information in question constitutes illegal content, as alleged. The obligation to put in place notice and action mechanisms should apply, for instance, to file storage and sharing services, web hosting services, advertising servers and paste bins, in so far as they qualify as hosting services covered by this Regulation. Recital 51 – Having regard to the need to take due account of the fundamental rights guaranteed under the Charter of all parties concerned, any action taken by a provider of hosting services pursuant to receiving a notice should be strictly targeted, in the sense that it should serve to remove or disable access to the specific items of information considered to constitute illegal content, without unduly affecting the freedom of expression and of information of recipients of the service. Notices should therefore, as a general rule, be directed to the providers of hosting services that can reasonably be expected to have the technical and operational ability to act against such specific items. The providers of hosting services who receive a notice for which they cannot, for technical or operational reasons, remove the specific item of information should inform the person or entity who submitted the notice." Recital 52 - The rules on such notice and action mechanisms should be harmonised at Union level, so as to provide for the timely, diligent and non-arbitrary processing of notices on the basis of rules that are uniform, transparent and clear and that provide for robust safeguards to protect the right and legitimate interests of all affected parties, in particular their fundamental rights guaranteed by the Charter, irrespective of the Member State in which those parties are established or reside and of the field of law at issue. Those fundamental rights include but are not limited to: for the recipients of the service, the right to freedom of expression and of information, the right to respect for private and family life, the right to protection of personal data, the right to non-discrimination and the right to an effective remedy; for the service providers, the freedom to conduct a business, including the freedom of contract; for parties affected by illegal content, the right to human dignity, the rights of the child, the right to protection of property, including intellectual property, and the right to non-discrimination. Providers of hosting services should act upon notices in a timely manner, in particular by taking into account the type of illegal content being notified and the urgency of taking action. For instance, such providers can be expected to act without delay when allegedly illegal content involving a threat to life or safety of persons is being notified. The provider of hosting services should inform the individual or entity notifying the specific content without undue delay after taking a decision whether or not to act upon the notice. Recital 53 – The notice and action mechanisms should allow for the submission of notices which are sufficiently precise and adequately substantiated to enable the provider of hosting services concerned to take an informed and diligent decision, compatible with the freedom of expression and of information, in respect of the content to which the notice relates, in particular whether or not that content is to be considered illegal content and is to be removed or access thereto is to be disabled. Those mechanisms should be such as to facilitate the provision of notices that contain an explanation of the reasons why the individual or the entity submitting a notice considers that content to be illegal content, and a clear indication of the location of that content. Where a notice contains sufficient information to enable a diligent provider of hosting services to identify, without a detailed legal examination, that it is clear that the content is illegal, the notice should be considered to give rise to actual knowledge or awareness of illegality. Except for the submission of notices relating to offences referred to in Articles 3 to 7 of Directive 2011/93/EU of the European Parliament and of the Council, those mechanisms should ask the individual or the entity submitting a notice to disclose its identity in order to avoid misuse.
    key_requirements:
      - Implement electronic notice submission system
      - Make mechanism easy to access and user-friendly
      - Allow submission of precise location of content (URL)
      - Allow explanation of why content is illegal
      - Process notices in timely, diligent, non-arbitrary manner
      - Confirm receipt of notice to submitter

  - article: 17
    title: Statement of reasons
    context: |
      When you restrict content or accounts, you must explain WHY. Users and 
      content uploaders deserve to know the specific reasons for restrictions.
      This enables meaningful appeals.
    key_requirements:
      - Provide clear statement of reasons for any restriction
      - Specify legal ground or ToS provision violated
      - Explain facts and circumstances of the decision
      - Include information about redress options
      - Deliver statement to affected user

  - article: 18
    title: Notification of suspicions of criminal offences
    context: |
      Recital 56 – Obligation to Report Criminal Offences:"A provider of hosting services may in some instances become aware, such as through a notice by a notifying party or through its own voluntary measures, of information relating to certain activity of a recipient of the service, such as the provision of certain types of illegal content, that reasonably justify, having regard to all relevant circumstances of which the provider of hosting services is aware, the suspicion that that recipient may have committed, may be committing or is likely to commit a criminal offence involving a threat to the life or safety of person or persons, such as offences specified in Directive 2011/36/EU of the European Parliament and of the Council, Directive 2011/93/EU or Directive (EU) 2017/541 of the European Parliament and of the Council. For example, specific items of content could give rise to a suspicion of a threat to the public, such as incitement to terrorism within the meaning of Article 21 of Directive (EU) 2017/541. In such instances, the provider of hosting services should inform without delay the competent law enforcement authorities of such suspicion. The provider of hosting services should provide all relevant information available to it, including, where relevant, the content in question and, if available, the time when the content was published, including the designated time zone, an explanation of its suspicion and the information necessary to locate and identify the relevant recipient of the service. This Regulation does not provide the legal basis for profiling of recipients of the services with a view to the possible identification of criminal offences by providers of hosting services. Providers of hosting services should also respect other applicable rules of Union or national law for the protection of the rights and freedoms of individuals when informing law enforcement authorities.
    key_requirements:
      - Establish process for identifying serious criminal threats
      - Promptly notify relevant law enforcement
      - Provide all relevant available information
      - This applies regardless of whether content was reported by users

  # ============================================================================
  # Section 3: Online Platform Obligations (Articles 19-28)
  # Apply to: Online Platform, Online Marketplace, Search Engine
  # NOTE: Articles 20-28 have SME exemption (unless marketplace or VLOP/VLOSE)
  # ============================================================================

  - article: 19
    title: SME Exemption for Platform Obligations
    context: |
      Small/micro enterprises (< 50 employees, < €10M turnover) are EXEMPT from 
      Section 3 obligations (Articles 20-28) UNLESS they are marketplaces or VLOPs.
      This article defines the exemption criteria.
    key_requirements:
      - Exemption applies to Articles 20-28
      - Must have < 50 employees AND < €10M annual turnover
      - Does NOT apply to marketplaces (Art. 30-32 always apply)
      - Does NOT apply to VLOPs/VLOSEs

  - article: 20
    title: Internal complaint-handling system
    context: |
      (58) Recipients of the service should be able to easily and effectively contest certain decisions of providers of online platforms concerning the illegality of content or its incompatibility with the terms and conditions that negatively affect them. Therefore, providers of online platforms should be required to provide for internal complaint-handling systems, which meet certain conditions that aim to ensure that the systems are easily accessible and lead to swift, non-discriminatory, non-arbitrary and fair outcomes, and are subject to human review where automated means are used. Such systems should enable all recipients of the service to lodge a complaint and should not set formal requirements, such as referral to specific, relevant legal provisions or elaborate legal explanations. Recipients of the service who submitted a notice through the notice and action mechanism provided for in this Regulation or through the notification mechanism for content that violate the terms and conditions of the provider of online platforms should be entitled to use the complaint mechanism to contest the decision of the provider of online platforms on their notices, including when they consider that the action taken by that provider was not adequate. The possibility to lodge a complaint for the reversal of the contested decisions should be available for at least six months, to be calculated from the moment at which the provider of online platforms informed the recipient of the service of the decision.
      (59) In addition, provision should be made for the possibility of engaging, in good faith, in the out-of-court dispute settlement of such disputes, including those that could not be resolved in a satisfactory manner through the internal complaint-handling systems, by certified bodies that have the requisite independence, means and expertise to carry out their activities in a fair, swift and cost-effective manner. The independence of the out-of-court dispute settlement bodies should be ensured also at the level of the natural persons in charge of resolving disputes, including through rules on conflict of interest. The fees charged by the out-of-court dispute settlement bodies should be reasonable, accessible, attractive, inexpensive for consumers and proportionate, and assessed on a case-by-case basis. Where an out-of-court dispute settlement body is certified by the competent Digital Services Coordinator, that certification should be valid in all Member States. Providers of online platforms should be able to refuse to engage in out-of-court dispute settlement procedures under this Regulation when the same dispute, in particular as regards the information concerned and the grounds for taking the contested decision, the effects of the decision and the grounds raised for contesting the decision, has already been resolved by or is already subject to an ongoing procedure before the competent court or before another competent out-of-court dispute settlement body. Recipients of the service should be able to choose between the internal complaint mechanism, an out-of-court dispute settlement and the possibility to initiate, at any stage, judicial proceedings. Since the outcome of the out-of-court dispute settlement procedure is not binding, the parties should not be prevented from initiating judicial proceedings in relation to the same dispute. The possibilities to contest decisions of providers of online platforms thus created should leave unaffected in all respects the possibility to seek judicial redress in accordance with the laws of the Member State concerned, and therefore should not affect the exercise of the right to an effective judicial remedy under Article 47 of the Charter. The provisions in this Regulation on out-of-court dispute settlement should not require Member States to establish such out-of-court settlement bodies.
    key_requirements:
      - Free, electronic, easy-to-use complaint system
      - Allow complaints against any content moderation decision
      - Handle complaints in timely, non-discriminatory manner
      - Provide human review (not purely automated)
      - Inform users of outcome and redress options

  - article: 21
    title: Out-of-court dispute settlement
    context: |
      (59) In addition, provision should be made for the possibility of engaging, in good faith, in the out-of-court dispute settlement of such disputes, including those that could not be resolved in a satisfactory manner through the internal complaint-handling systems, by certified bodies that have the requisite independence, means and expertise to carry out their activities in a fair, swift and cost-effective manner. The independence of the out-of-court dispute settlement bodies should be ensured also at the level of the natural persons in charge of resolving disputes, including through rules on conflict of interest. The fees charged by the out-of-court dispute settlement bodies should be reasonable, accessible, attractive, inexpensive for consumers and proportionate, and assessed on a case-by-case basis. Where an out-of-court dispute settlement body is certified by the competent Digital Services Coordinator, that certification should be valid in all Member States. Providers of online platforms should be able to refuse to engage in out-of-court dispute settlement procedures under this Regulation when the same dispute, in particular as regards the information concerned and the grounds for taking the contested decision, the effects of the decision and the grounds raised for contesting the decision, has already been resolved by or is already subject to an ongoing procedure before the competent court or before another competent out-of-court dispute settlement body. Recipients of the service should be able to choose between the internal complaint mechanism, an out-of-court dispute settlement and the possibility to initiate, at any stage, judicial proceedings. Since the outcome of the out-of-court dispute settlement procedure is not binding, the parties should not be prevented from initiating judicial proceedings in relation to the same dispute. The possibilities to contest decisions of providers of online platforms thus created should leave unaffected in all respects the possibility to seek judicial redress in accordance with the laws of the Member State concerned, and therefore should not affect the exercise of the right to an effective judicial remedy under Article 47 of the Charter. The provisions in this Regulation on out-of-court dispute settlement should not require Member States to establish such out-of-court settlement bodies.
      (60) For contractual consumer-to-business disputes regarding the purchase of goods or services, Directive 2013/11/EU ensures that Union consumers and businesses in the Union have access to quality-certified alternative dispute resolution entities. In this regard, it should be clarified that the rules of this Regulation on out-of-court dispute settlement are without prejudice to that Directive, including the right of consumers under that Directive to withdraw from the procedure at any stage if they are dissatisfied with the performance or the operation of the procedure.
    key_requirements:
      - Enable users to select certified dispute settlement body
      - Engage in good faith with selected body
      - Accept binding decisions from settlement body
      - Bear reasonable costs for users who prevail

  - article: 22
    title: Trusted flaggers
    context: |
      (61) Action against illegal content can be taken more quickly and reliably where providers of online platforms take the necessary measures to ensure that notices submitted by trusted flaggers, acting within their designated area of expertise, through the notice and action mechanisms required by this Regulation are treated with priority, without prejudice to the requirement to process and decide upon all notices submitted under those mechanisms in a timely, diligent and non-arbitrary manner. Such trusted flagger status should be awarded by the Digital Services Coordinator of the Member State in which the applicant is established and should be recognised by all providers of online platforms within the scope of this Regulation. Such trusted flagger status should only be awarded to entities, and not individuals, that have demonstrated, among other things, that they have particular expertise and competence in tackling illegal content and that they work in a diligent, accurate and objective manner. Such entities can be public in nature, such as, for terrorist content, internet referral units of national law enforcement authorities or of the European Union Agency for Law Enforcement Cooperation (Europol) or they can be non-governmental organisations and private or semi-public bodies such as the organisations part of the INHOPE network of hotlines for reporting child sexual abuse material and organisations committed to notifying illegal racist and xenophobic expressions online. To avoid diminishing the added value of such mechanism, the overall number of trusted flaggers awarded in accordance with this Regulation should be limited. In particular, industry associations representing their members' interests are encouraged to apply for the status of trusted flaggers, without prejudice to the right of private entities or individuals to enter into bilateral agreements with the providers of online platforms.
      (62) Trusted flaggers should publish easily comprehensible and detailed reports on notices submitted in accordance with this Regulation. Those reports should indicate information such as the number of notices categorised by the provider of hosting services, the type of content, and the action taken by the provider. Given that trusted flaggers have demonstrated expertise and competence, the processing of notices submitted by trusted flaggers can be expected to be less burdensome and therefore faster compared to notices submitted by other recipients of the service. However, the average time taken to process may still vary depending on factors including the type of illegal content, the quality of notices, and the actual technical procedures put in place for the submission of such notices. For example, while the Code of conduct on countering illegal hate speech online of 2016 sets a benchmark for the participating companies with respect to the time needed to process valid notifications for removal of illegal hate speech, other types of illegal content may take considerably different timelines for processing, depending on the specific facts and circumstances and types of illegal content at stake. In order to avoid abuses of the trusted flagger status, it should be possible to suspend such status when a Digital Services Coordinator of establishment opened an investigation based on legitimate reasons. The rules of this Regulation on trusted flaggers should not be understood to prevent providers of online platforms from giving similar treatment to notices submitted by entities or individuals that have not been awarded trusted flagger status under this Regulation, from otherwise cooperating with other entities, in accordance with the applicable law, including this Regulation and Regulation (EU) 2016/794 of the European Parliament and of the Council (29). The rules of this Regulation should not prevent the providers of online platforms from making use of such trusted flagger or similar mechanisms to take quick and reliable action against content that is incompatible with their terms and conditions, in particular against content that is harmful for vulnerable recipients of the service, such as minors.
    key_requirements:
      - Process trusted flagger notices with priority
      - Ensure timely decisions on trusted flagger notices
      - Maintain respectful cooperation with trusted flaggers

  - article: 23
    title: Measures and protection against misuse
    context: |
      (63) The misuse of online platforms by frequently providing manifestly illegal content or by frequently submitting manifestly unfounded notices or complaints under the mechanisms and systems, respectively, established under this Regulation undermines trust and harms the rights and legitimate interests of the parties concerned. Therefore, there is a need to put in place appropriate, proportionate and effective safeguards against such misuse, that need to respect the rights and legitimate interests of all parties involved, including the applicable fundamental rights and freedoms as enshrined in the Charter, in particular the freedom of expression. Information should be considered to be manifestly illegal content and notices or complaints should be considered manifestly unfounded where it is evident to a layperson, without any substantive analysis, that the content is illegal or, respectively, that the notices or complaints are unfounded.
      (64) Under certain conditions, providers of online platforms should temporarily suspend their relevant activities in respect of the person engaged in abusive behaviour. This is without prejudice to the freedom by providers of online platforms to determine their terms and conditions and establish stricter measures in the case of manifestly illegal content related to serious crimes, such as child sexual abuse material. For reasons of transparency, this possibility should be set out, clearly and in sufficient detail, in the terms and conditions of the online platforms. Redress should always be open to the decisions taken in this regard by providers of online platforms and they should be subject to oversight by the competent Digital Services Coordinator. Providers of online platforms should send a prior warning before deciding on the suspension, which should include the reasons for the possible suspension and the means of redress against the decision of the providers of the online platform. When deciding on the suspension, providers of online platforms should send the statement of reasons in accordance with the rules set out in this Regulation. The rules of this Regulation on misuse should not prevent providers of online platforms from taking other measures to address the provision of illegal content by recipients of their service or other misuse of their services, including through the violation of their terms and conditions, in accordance with the applicable Union and national law. Those rules are without prejudice to any possibility to hold the persons engaged in misuse liable, including for damages, provided for in Union or national law.
    key_requirements:
      - Suspend processing for repeat abusers
      - Establish clear policy on misuse
      - Issue warnings before suspension
      - Maintain proportionality in enforcement

  - article: 24
    title: Transparency reporting (enhanced for platforms)
    context: |
      (65) In addition to the transparency reporting obligations applicable to providers of intermediary services, providers of online platforms should publish information on the outcomes of the internal complaint-handling systems and the out-of-court dispute settlement referred to in this Regulation. They should also publish information on the number of suspensions carried out under this Regulation in respect of recipients of the service who frequently provide manifestly illegal content or who frequently submit manifestly unfounded notices or complaints. To enable detection and designation of very large online platforms and very large online search engines, providers of online platforms or of online search engines should be required to make public the average monthly active recipients of the service in the Union. The average monthly active recipients of the service in the Union should be calculated on the basis of a six-month period, which should ensure that one-off spikes in usage do not trigger the obligations applicable to very large online platforms and of very large online search engines. It is also necessary to provide for a mechanism to confirm the accuracy of the data published in this regard and, where necessary, of the calculations to ensure compliance with this Regulation.
      (66) Providers of online platforms should systematically communicate, without undue delay, all the decisions and statements of reasons, as referred to in this Regulation, that they take in respect of information provided by the recipients of the service, to the Commission. That information should be submitted for inclusion in a publicly available database managed by the Commission. To avoid information that can identify individuals from being made available to the public, providers of online platforms should ensure that the information submitted does not contain any personal data. That information should be collected for the purposes of transparency and enforcement of this Regulation. It can, for instance, enable interested parties, such as researchers and civil society organisations, to scrutinise the enforcement of this Regulation and to understand the procedures and practices of the online platforms as regards content moderation. That information can also enable the Commission to better design additional tools or templates for reporting, or to provide technical guidance.
    key_requirements:
      - All Art. 15 requirements PLUS additional data
      - Report on complaint-handling system usage
      - Report on out-of-court dispute outcomes
      - Include automated moderation statistics
      - Publish every 6 months (not annually)
      - Note: Article 24(3) is NOT exempt for SMEs (see separate entry)

  - article: "24.3"
    title: Information on average monthly active recipients (non-exempt for SMEs)
    context: |
      Article 24(3) requires providers of online platforms to furnish information 
      about their average monthly active recipients upon request from the Digital 
      Services Coordinator or the European Commission. This obligation applies 
      regardless of the enterprise's size - even micro and small platforms must 
      provide this data for regulatory oversight.
    key_requirements:
      - Provide information on average monthly active recipients upon request
      - Respond to requests from Digital Services Coordinator
      - Respond to requests from European Commission
      - This obligation is NOT exempt for SMEs (unlike other parts of Article 24)

  - article: 25
    title: Online interface design and organisation
    context: |
      (67) Providers of online platforms should neither use so-called dark patterns to deceive users nor directly or indirectly subvert or impair the autonomy, decision-making or choice of the recipients of the service, including by means of their online interface. Examples of such practices include making the cancellation of a service more difficult than signing up to it, or making certain choices more prominent than others through visual, auditory or other elements. Those practices can be particularly detrimental to minors and can be associated with mental health issues, and with marginalised groups, and can also be used to encourage harmful behaviour or content. In accordance with this Regulation, it should, nevertheless, be possible to recommend information as part of a specific information society service, also where the recommendation is carried out through ranking, rating, review or search functions, including ordering of information, also where it is prioritised or given more relative prominence by means of visual, auditory or other elements. That information should, in particular, be provided in an easily comprehensible and legible manner. Where such practices are already prohibited by applicable Union law such as Directive 2005/29/EC on unfair business-to-consumer commercial practices in the internal market or Regulation (EU) 2016/679 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data (General Data Protection Regulation), the rules set out in this Regulation in this regard should not apply to such practices, without prejudice to the power of Digital Services Coordinators to take measures pursuant to this Regulation as regards compliance with this Regulation.
    key_requirements:
      - No deceptive interface design
      - No manipulation to influence decisions
      - Clear presentation of choices
      - No making certain choices harder than others
      - Advertising must be clearly identifiable

  - article: 26
    title: Advertising on online platforms
    context: |
      (68) In order to increase transparency and to create a safe and predictable environment for recipients of the service, and to prevent unfair commercial practices on online platforms, this Regulation should require providers of online platforms that display advertising on their online interfaces to require those recipients to be able to identify clearly and unambiguously that the information they are viewing is an advertisement. Requirements to identify advertising also apply in relation to advertisements presented as content by recipients of the service themselves, where such recipients have a commercial relationship with the online platform. In particular, influencers on audio or audio-visual media sharing platforms could be subject to such requirements. In order to avoid imposing unnecessary burdens on recipients who are not posting advertisements, the content should be identified as commercial communications only where such recipients have declared it to be so. To that end, online platforms should provide a user-friendly functionality for such declarations to be submitted. In order to facilitate supervision of compliance with this Regulation and to create a high level of transparency on online platforms, information displayed as advertisements, including by so-called influencers, should always clearly and prominently indicate the natural or legal person on whose behalf such advertisement is presented and the natural or legal person who paid for such advertisement, if different from the natural or legal person on whose behalf it is presented. In addition, in order to increase transparency in relation to advertising displayed on online platforms and to enhance the autonomy of recipients of the service, it is appropriate to require online platforms to provide meaningful information regarding the criteria that underpin the advertisement being displayed to that particular recipient, based on profiling or other processing of personal data or without such processing. Such information should be provided by the provider of the online platform in a clear and comprehensive manner, enabling recipients of the service to know what factors they should change or not in order to influence whether or not they will be presented with such an advertisement. In this regard, under Article 21(5) of Regulation (EU) 2016/679, the data subject has the right to object at any time to profiling activities for the purposes of direct marketing. In addition, Article 6(1) of Directive 2002/58/EC provides that consent is required for the storage of information, and gaining of access to information already stored, in the terminal equipment of a user or subscriber. Information about the main parameters can include, for example, that an advertisement is being presented to the recipient of the service on the basis of the fact that the recipient is situated in a particular location, whether inferred or known, the language in which the recipient's online interface is displayed, or the type of device used by the recipient.
      (69) In order to protect minors and to respect their right to protection of their best interests, and considering that minors are generally less capable of assessing the consequences of being profiled and of defending themselves, it is necessary to prohibit the presentation of advertisements on online platforms based on profiling using personal data of the recipient of the service when the provider of online platforms has information that allows it to conclude, with reasonable certainty, that the recipient of the service is a minor. The principle of the best interests of the child should be a primary consideration by providers of online platforms when designing, developing and operating their services.
      (70) The prohibition of presenting targeted advertising on the basis of special categories of personal data, in line with Article 9(1) of Regulation (EU) 2016/679, enhances the protection of recipients of the service as regards the risks that such advertising can create.
    key_requirements:
      - Clearly mark content as advertising
      - Identify advertiser (or who paid)
      - Provide meaningful targeting information
      - Real-time, visible, and accessible information

  - article: 27
    title: Recommender system transparency
    context: |
      (71) Recommender systems play an important role in the amplification of certain messages and in the viral dissemination of information, thereby having a significant impact on the ability of recipients of the service to retrieve and interact with information online. Such systems can also significantly influence the online debate, and the choices of recipients of the service, for instance what media they consume, and to have an impact on other persons' fundamental rights, personal security, and the functioning of democratic processes and public security. Such systems should, therefore, be transparent and recipients of the service should be provided with appropriate levels of transparency and control over the functioning of these systems. While recommender systems used by online platforms can take various forms, the rules of this Regulation should apply to all of them, in order to ensure a high level of transparency and accountability. This Regulation should not affect recommender systems that are deployed for the sole purpose of facilitating the requested or initiated task by the recipient of the service. In particular, this includes baseline recommender systems including, for example, recommender systems that merely sort or organise information provided by recipients of the service based on parameters explicitly requested or selected by the recipient of the service, such as alphabetical ordering or chronological ordering, and lists based on explicit choices by the recipients such as filters and or search tools. Such baseline recommender systems should thus not fall within the scope of the transparency obligation for recommender systems laid down in this Regulation, as they are limited to organising information in a manner requested by the recipient of the service and do not use prioritisation using means beyond such explicit requests. To ensure the transparency and accountability of automated recommender systems that play a crucial role in mediating access to information and communications, providers of online platforms should clearly present to recipients of their service the main parameters for recommender systems in their terms and conditions, along with any options for modifying or influencing those parameters. That information should be clear and easily comprehensible for recipients and explain the parameters that determine the information presented to recipients of the service. In addition, online platforms should provide recipients of the service with at least one option for each of their recommender systems, enabling them to modify or influence those parameters.
    key_requirements:
      - Explain main parameters of recommender systems
      - Provide this in ToS in accessible manner
      - Offer non-profiled recommendation option
      - Allow users to modify recommendation parameters

  - article: 28
    title: Online protection of minors
    context: |
      (69) In order to protect minors and to respect their right to protection of their best interests, and considering that minors are generally less capable of assessing the consequences of being profiled and of defending themselves, it is necessary to prohibit the presentation of advertisements on online platforms based on profiling using personal data of the recipient of the service when the provider of online platforms has information that allows it to conclude, with reasonable certainty, that the recipient of the service is a minor. The principle of the best interests of the child should be a primary consideration by providers of online platforms when designing, developing and operating their services.
    key_requirements:
      - Design services with minor protection in mind
      - Implement appropriate safeguards
      - No targeted advertising based on profiling to known minors
      - High level of privacy and safety for minors

  - article: 29
    title: SME Exemption for Marketplace Obligations
    context: |
      (71) Recommender systems play an important role in the amplification of certain messages and in the viral dissemination of information, thereby having a significant impact on the ability of recipients of the service to retrieve and interact with information online. Such systems can also significantly influence the online debate, and the choices of recipients of the service, for instance what media they consume, and to have an impact on other persons' fundamental rights, personal security, and the functioning of democratic processes and public security. Such systems should, therefore, be transparent and recipients of the service should be provided with appropriate levels of transparency and control over the functioning of these systems. While recommender systems used by online platforms can take various forms, the rules of this Regulation should apply to all of them, in order to ensure a high level of transparency and accountability. This Regulation should not affect recommender systems that are deployed for the sole purpose of facilitating the requested or initiated task by the recipient of the service. In particular, this includes baseline recommender systems including, for example, recommender systems that merely sort or organise information provided by recipients of the service based on parameters explicitly requested or selected by the recipient of the service, such as alphabetical ordering or chronological ordering, and lists based on explicit choices by the recipients such as filters and or search tools. Such baseline recommender systems should thus not fall within the scope of the transparency obligation for recommender systems laid down in this Regulation, as they are limited to organising information in a manner requested by the recipient of the service and do not use prioritisation using means beyond such explicit requests. To ensure the transparency and accountability of automated recommender systems that play a crucial role in mediating access to information and communications, providers of online platforms should clearly present to recipients of their service the main parameters for recommender systems in their terms and conditions, along with any options for modifying or influencing those parameters. That information should be clear and easily comprehensible for recipients and explain the parameters that determine the information presented to recipients of the service. In addition, online platforms should provide recipients of the service with at least one option for each of their recommender systems, enabling them to modify or influence those parameters. The protection of minors is an important policy objective of the Union. An online platform can be considered to be accessible to minors when its terms and conditions permit minors to use the service, when its service is directed at or predominantly used by minors, or where the provider is otherwise aware that some of the recipients of its service are minors, for example because it already processes personal data of the recipients of its service revealing their age for other purposes. Providers of online platforms used by minors should take appropriate and proportionate measures to protect minors, for example by designing their online interfaces or parts thereof with the highest level of privacy, safety and security for minors by default where appropriate or adopting standards for protection of minors, or participating in codes of conduct for protecting minors. They should consider best practices and available guidance, such as that provided by the communication of the Commission on 'A Digital Decade for children and youth: the new European strategy for a better internet for kids (BIK+)'. Providers of online platforms should not present advertisements based on profiling using personal data of the recipient of the service when they are aware with reasonable certainty that the recipient of the service is a minor. In accordance with Regulation (EU) 2016/679, notably the principle of data minimisation as provided for in Article 5(1), point (c), thereof, this prohibition should not lead the provider of the online platform to maintain, acquire or process more personal data than it already has in order to assess if the recipient of the service is a minor. Thus, this obligation should not incentivize providers of online platforms to collect the age of the recipient of the service prior to their use. It should be without prejudice to Union law on protection of personal data.
    key_requirements:
      - No SME exemption for Articles 30-32
      - All marketplaces must comply regardless of size
      - This ensures consumer protection and trader traceability

  # ============================================================================
  # Section 4: Online Marketplace Obligations (Articles 30-32)
  # Apply to: Online Marketplace only
  # NOTE: NO SME exemption for marketplace obligations
  # ============================================================================

  - article: 30
    title: Traceability of traders
    context: |
      (72) In order to contribute to a safe, trustworthy and transparent online environment for consumers, as well as for other interested parties such as competing traders and holders of intellectual property rights, and to deter traders from selling products or services in violation of the applicable rules, online platforms allowing consumers to conclude distance contracts with traders should ensure that such traders are traceable. The trader should therefore be required to provide certain essential information to the providers of online platforms allowing consumers to conclude distance contracts with traders, including for purposes of promoting messages on or offering products. That requirement should also be applicable to traders that promote messages on products or services on behalf of brands, based on underlying agreements. Those providers of online platforms should store all information in a secure manner for the duration of their contractual relationship with the trader and 6 months thereafter, to allow any claims to be filed against the trader or orders related to the trader to be complied with. This obligation is necessary and proportionate, so that the information can be accessed, in accordance with the applicable law, including on the protection of personal data, by public authorities and private parties with a legitimate interest, including through the orders to provide information referred to in this Regulation. This obligation leaves unaffected potential obligations to preserve certain content for longer periods of time, on the basis of other Union law or national laws, in compliance with Union law. Without prejudice to the definition provided for in this Regulation, any trader, irrespective of whether it is a natural or legal person, identified on the basis of Article 6a(1), point (b), of Directive 2011/83/EU and Article 7(4), point (f), of Directive 2005/29/EC should be traceable when offering a product or service through an online platform. Directive 2000/31/EC obliges all information society services providers to render easily, directly and permanently accessible to the recipients of the service and competent authorities certain information allowing the identification of all providers. The traceability requirements for providers of online platforms allowing consumers to conclude distance contracts with traders set out in this Regulation do not affect the application of Council Directive (EU) 2021/514 (30), which pursues other legitimate public interest objectives.
      (73) To ensure an efficient and adequate application of that obligation, without imposing any disproportionate burdens, providers of online platforms allowing consumers to conclude distance contracts with traders should make best efforts to assess the reliability of the information provided by the traders concerned, in particular by using freely available official online databases and online interfaces, such as national trade registers and the VAT Information Exchange System, or request the traders concerned to provide trustworthy supporting documents, such as copies of identity documents, certified payment accounts statements, company certificates and trade register certificates. They may also use other sources, available for use at a distance, which offer a similar degree of reliability for the purpose of complying with this obligation. However, the providers of online platforms concerned should not be required to engage in excessive or costly online fact-finding exercises or to carry out disproportionate verifications on the spot. Nor should such providers, which have made the best efforts required by this Regulation, be understood as guaranteeing the reliability of the information towards consumer or other interested parties.(74) Given the ease and speed with which illegal products can be offered for sale online, it is important to establish harmonised due diligence obligations for providers of online platforms allowing consumers to conclude distance contracts with traders, so-called 'online marketplaces'. Those obligations should aim to ensure that traders, before they are allowed to offer their products or services through that online platforms, provide basic and reliable information to those providers, in order to make it possible to identify and contact them ex post, as well as identify the specific products or services offered. The basic information should include the name, address, telephone number and email address of the trader; a copy of the identification document of the trader or any other form of electronic identification or European Digital Identity Wallet, as defined by Regulation (EU) No 910/2014 of the European Parliament and of the Council (39); the payment account details of the trader; where the trader is registered in a trade register or similar public register, the trade register in which the trader is registered and its registration number or equivalent means of identification in that register; and a self-certification by the trader committing to only offer products or services that comply with the applicable rules of Union law. Providers of online platforms allowing consumers to conclude distance contracts with traders should make best efforts to verify that the information is reliable, for example by checking official online databases. The online interfaces should be user-friendly and easily accessible for traders and consumers. Additionally and after allowing the offering of the product or service by the trader, the providers of online platforms concerned should make reasonable efforts to randomly check whether the products or services offered have been identified as being illegal in any official, freely accessible and machine-readable online databases or online interfaces available in a Member State or in the Union. The Commission should also encourage traceability of products through technology solutions such as digitally signed Quick Response codes (or 'QR codes') or non-fungible tokens. The Commission should promote the development of standards and, in the absence of them, of market led solutions which can be acceptable to the parties concerned.
    key_requirements:
      - Collect trader name, address, ID, contact details
      - Collect trade register and VAT information
      - Collect bank account details
      - Make best efforts to verify information accuracy
      - Verify against official databases where possible
      - Obtain trader self-certification of compliance
      - Request updated info if reason to believe data is incorrect

  - article: 31
    title: Compliance by design
    context: |
      Your platform interface must be designed to help traders comply with EU law.
      Make it easy to provide required pre-contractual information, product safety
      data, and trader identification.
    key_requirements:
      - Enable traders to provide legally required information
      - Design interface to facilitate compliance
      - Include fields for product/service identification
      - Include fields for trader identification
      - Ensure information is displayed to consumers

  - article: 32
    title: Right to information
    context: |
      Recital (76)"In order to ensure a safe online marketplace environment and to limit the exposure of consumers to illegal or unsafe products and services, providers of online platforms allowing consumers to conclude distance contracts with traders should directly inform consumers who purchased a product or service through their online interface where that content has been removed or disabled on the grounds that it is illegal or pursuant to an order to remove illegal content. That information should include data on the reasons for the removal or disabling of access, information on the identity of the trader and information on possibilities for redress, to allow consumers who may be entitled to compensation to identify the relevant trader and to contact that trader where needed. Where available, other relevant information that was collected during the vetting procedure according to Article 30 of this Regulation should also be provided. Consumers should be informed without undue delay, but in any event within a maximum of 14 days, so that they are able to seek appropriate remedies, such as compensation for damages suffered as a result of the purchase of the illegal or unsafe product or the use of the illegal service. However, that information should only be provided where the provider of the online platform allowing consumers to conclude distance contracts with traders holds the contact details of the consumer. This Regulation is without prejudice to the rules of national laws on the liability of traders for defective products and without prejudice to consumer rights stemming from Union or national consumer law."
    key_requirements:
      - Notify consumers of illegal products purchased
      - Provide identity of the trader
      - Provide relevant information for redress
      - Make best efforts to reach affected consumers
      - Notification must be clear and comprehensible

  # ============================================================================
  # Section 5: VLOP/VLOSE Obligations (Articles 33-43)
  # Apply to: Very Large Online Platforms (VLOPs) and Very Large Online Search Engines (VLOSEs)
  # Threshold: ≥45 million average monthly active recipients in the EU
  # ============================================================================

  - article: 33
    title: Designation of VLOPs and VLOSEs
    context: |
      Article 33 establishes the criteria and procedures for designating online platforms 
      and search engines as Very Large Online Platforms (VLOPs) and Very Large Online 
      Search Engines (VLOSEs). Services with ≥45 million average monthly active 
      recipients in the EU are subject to designation. Providers must publish user 
      numbers and comply with designation procedures.
    key_requirements:
      - Publish EU user numbers prominently if approaching or exceeding 45M threshold
      - Update user numbers every 6 months
      - Cooperate with Commission designation process
      - Can apply for designation if meeting criteria
      - Can apply for revocation if no longer meeting criteria
      - Comply with designation within 4 months of designation

  - article: 34
    title: Risk assessment
    context: |
      VLOPs/VLOSEs must conduct comprehensive ANNUAL assessments of systemic risks
      arising from their services. This includes risks to fundamental rights,
      civic discourse, electoral processes, public health, and minors.
    key_requirements:
      - Annual systemic risk assessment
      - Assess illegal content dissemination risks
      - Assess negative effects on fundamental rights
      - Assess risks to civic discourse and elections
      - Assess risks to public health and minors
      - Consider recommender systems and advertising in assessment
      - Consider content moderation and ToS enforcement

  - article: 35
    title: Mitigation of risks
    context: |
      Based on risk assessments, implement reasonable, proportionate, and effective
      mitigation measures. This is the "do something about it" requirement.
    key_requirements:
      - Implement mitigation measures for identified risks
      - Adapt content moderation as needed
      - Adjust recommender systems if contributing to risk
      - Adapt advertising systems if needed
      - Enhance internal processes and resources
      - Cooperate with trusted flaggers and authorities
      - Launch awareness initiatives

  - article: 36
    title: Crisis response mechanism
    context: |
      During extraordinary crises (war, pandemic, natural disaster), the Commission
      can require VLOPs/VLOSEs to take specific emergency measures. Must be ready
      to respond rapidly.
    key_requirements:
      - Respond to Commission crisis requests
      - Assess contribution of service to the crisis
      - Implement proportionate emergency measures
      - Report on actions taken

  - article: 37
    title: Independent audit
    context: |
      ANNUAL independent audits are MANDATORY. Auditors verify compliance with
      all DSA obligations. Audit reports become public. This is serious accountability.
    key_requirements:
      - Annual audit by independent organization
      - Auditor must be independent and expert
      - Provide auditor with full access and cooperation
      - Publish audit report
      - Implement audit recommendations or explain non-implementation
      - Audit compliance with ALL Chapter III obligations

  - article: 38
    title: Recommender systems (additional transparency)
    context: |
      Beyond Art. 27, VLOPs/VLOSEs must offer users the ability to EASILY modify
      or influence the main parameters of recommender systems. More granular control.
    key_requirements:
      - Provide easy parameter modification functionality
      - Offer at least one non-profiling option
      - Make options easily accessible in interface (not buried)
      - Allow users to change settings at any time

  - article: 39
    title: Additional online advertising transparency
    context: |
      VLOPs/VLOSEs must maintain a PUBLIC, searchable REPOSITORY of ALL ads
      shown on their platform. Researchers and regulators can monitor advertising.
    key_requirements:
      - Compile and publish ad repository
      - Repository must be publicly accessible
      - Include ad content
      - Include advertiser identity
      - Include targeting parameters used
      - Include total views/reach
      - Maintain repository for one year after last display

  - article: 40
    title: Data access for researchers
    context: |
      Vetted researchers get access to platform data for studying systemic risks.
      This is unprecedented transparency enabling academic scrutiny.
    key_requirements:
      - Provide data access to vetted researchers
      - Access must enable systemic risk research
      - Comply with Digital Services Coordinator requests
      - Implement technical infrastructure for data access
      - Protect user privacy while enabling research

  - article: 41
    title: Compliance function
    context: |
      A dedicated COMPLIANCE OFFICER must be appointed with independence and
      authority. Reports directly to management. Cannot be removed without cause.
    key_requirements:
      - Appoint independent compliance officer(s)
      - Ensure officer has necessary resources and authority
      - Officer reports directly to highest management level
      - Guarantee officer's independence
      - Officer monitors and ensures compliance

  - article: 42
    title: Transparency reporting (VLOP-enhanced)
    context: |
      The most detailed transparency reporting requirements. Includes risk
      assessment summaries, audit results, and detailed moderation data.
    key_requirements:
      - All previous reporting requirements
      - Publish risk assessment summaries
      - Publish audit implementation reports
      - More detailed content moderation statistics
      - Report on resources dedicated to moderation
      - Publish every 6 months

  - article: 43
    title: Supervisory fee
    context: |
      VLOPs/VLOSEs must pay an annual supervisory fee to the European Commission 
      to cover the costs of the Commission's supervisory tasks under the DSA. 
      The fee is proportionate to the number of average monthly active recipients 
      in the EU but shall not exceed 0.05% of the service provider's worldwide 
      annual net income.
    key_requirements:
      - Pay annual supervisory fee to European Commission
      - Fee covers Commission's supervisory and enforcement costs
      - Fee is proportionate to EU user numbers
      - Fee capped at 0.05% of worldwide annual net income
      - Fee based on preceding financial year's user numbers
