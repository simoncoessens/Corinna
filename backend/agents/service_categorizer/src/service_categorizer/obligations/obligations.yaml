# Consolidated DSA Obligations
# This file contains all obligations with mappings to service categories and size-based exemptions

# Service Categories:
# - Mere Conduit
# - Caching
# - Hosting
# - Online Platform
# - Online Marketplace
# - Search Engine

# Size Categories:
# - Very Large Platform (VLOP/VLOSE): ≥45M EU users
# - SME Exemption Eligible: <50 employees AND <€10M turnover

# =============================================================================
# Selection rules (authoritative)
#
# Logic:
# - Start with the base list for the service category (category_articles[category])
# - If VLOP/VLOSE: add extra articles (size_rules.vlop_vlose.extra_articles)
# - If SME exemption eligible: remove exemptable articles (size_rules.sme_exemption_eligible.exempt_articles)
#   but ONLY for Online Platforms that are NOT marketplaces and NOT VLOPs/VLOSEs.
# =============================================================================

category_articles:
  Mere Conduit: [ 11, 12, 13, 14, 15]
  Caching: [ 11, 12, 13, 14, 15]
  Hosting: [ 11, 12, 13, 14, 15, 16, 17, 18]
  Online Platform:
    [11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, "24.3", 25, 26, 27, 28]
  Online Marketplace:
    [ 
      11,
      12,
      13,
      14,
      15,
      16,
      17,
      18,
      20,
      21,
      22,
      23,
      24,
      "24.3",
      25,
      26,
      27,
      28,
      30,
      31,
      32,
    ]
  Search Engine: [9, 10, 11, 12, 13, 14, 15]

size_rules:
  # Size rules modify the *base* `category_articles` selection.
  #
  # How to use:
  # - **VLOP/VLOSE**: if `is_vlop_vlose == true`, we ADD `extra_articles` to the base list
  # - **SME exemption**: if `qualifies_for_sme_exemption == true`, we REMOVE `exempt_articles`
  #   from the base list (but only when the rule applies and the guards allow it).
  #
  # Important: SME exemptions differ by category:
  # - Online Platforms: SME-eligible providers can be exempt from Section 3 (Art. 20–28)
  # - Online Marketplaces: SME exemption does remove obligation in Section 3 (Art. 20–28), but in addition, they don't have the obligations of Section 4 (Article 30-32).
  #
  # Notes:
  # - We never infer category membership here; we only modify by article numbers.
  # - If you add a new obligation entry below, also update the relevant article lists here.
  vlop_vlose:
    # Only apply this rule when the *service category* is one of these.
    applicable_categories:
      ["Online Platform", "Online Marketplace", "Search Engine"]
    # Extra obligations that apply *in addition to* the base category when VLOP/VLOSE is true.
    extra_articles: [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]
  sme_exemption_online_platform:
    # SME exemption for Section 3 obligations (Art. 20–28) for *non-marketplace* online platforms DO NOT APPLY if the company is designated as VLOP/VLOSE.
    # SME exemption for Section 4 obligations (Art. 30–32) for marketplace online platforms DO NOT APPLY if the company is designated as VLOP/VLOSE.

    # Note: Article 24.3 is NOT exempt (must be excluded from exempt_articles).
    applicable_categories: ["Online Platform"]
    exempt_articles: [20, 21, 22, 23, 24, 25, 26, 27, 28]
    # Articles that are NOT exempt even when SME-eligible (e.g., "24.3")
    not_exempt_articles: ["24.3"]
    # Guard: if true, do NOT apply SME exemption removals when the service is a marketplace.
    not_if_marketplace: true
    # Guard: if true, do NOT apply SME exemption removals when the service is VLOP/VLOSE.
    not_if_vlop_vlose: true
  sme_exemption_online_marketplace:
    # Marketplaces benefit from the SME exemption for Section 3 (Art. 19(2)).
    # Keep this rule explicit so the distinction is visible in config; it removes nothing.
    applicable_categories: ["Online Marketplace"]
    exempt_articles: []
    not_if_marketplace: false
    not_if_vlop_vlose: true

obligations:
  # ============================================================================
  # Section 1: Intermediary Service Obligations (Articles 11-15)
  # Apply to: ALL intermediary services (Mere Conduit, Caching, Hosting,
  #           Online Platform, Online Marketplace, Search Engine)
  # ============================================================================


 
  - article: 11
    title: Points of contact for Member States' authorities
    context: |
      Recital 42  - In order to facilitate smooth and efficient two-way communications, including, where relevant, by acknowledging the receipt of such communications, relating to matters covered by this Regulation, providers of intermediary services should be required to designate a single electronic point of contact and to publish and update relevant information relating to that point of contact, including the languages to be used in such communications. The electronic point of contact can also be used by trusted flaggers and by professional entities which are under a specific relationship with the provider of intermediary services. In contrast to the legal representative, the electronic point of contact should serve operational purposes and should not be required to have a physical location. Providers of intermediary services can designate the same single point of contact for the requirements of this Regulation as well as for the purposes of other acts of Union law. When specifying the languages of communication, providers of intermediary services are encouraged to ensure that the languages chosen do not in themselves constitute an obstacle to communication. Where necessary, it should be possible for providers of intermediary services and Member States authorities to reach a separate agreement on the language of communication, or to seek alternative means to overcome the language barrier, including by using all available technological means or internal and external human resources.
      
    key_requirements:
      - Designate a single point of contact to enable direct communication by electronic means with Member States' authorities, the Commission, and the Board
      - Make the point of contact information publicly available, easily accessible, and kept up to date
      - Specify the official languages for communication, which must include at least one official language of the Member State where the provider has its main establishment
      - Ensure the point of contact allows for rapid and efficient exchanges and does not constitute an obstacle to communication

    edge_cases:
      - A provider cannot choose a language that excludes the official language of their main establishment Member State
      - The point of contact is an operational function and does not require a physical location in the Union, unlike the Legal Representative
      - This point of contact can be the same as designated for other acts of Union law

  - article: 12
    title: Points of contact for recipients of the service
    context: |
      Recital 43 - Providers of intermediary services should also be required to designate a single point of contact for recipients of services, enabling rapid, direct and efficient communication in particular by easily accessible means such as telephone numbers, email addresses, electronic contact forms, chatbots or instant messaging. It should be explicitly indicated when a recipient of the service communicates with chatbots. Providers of intermediary services should allow recipients of services to choose means of direct and efficient communication which do not solely rely on automated tools. Providers of intermediary services should make all reasonable efforts to guarantee that sufficient human and financial resources are allocated to ensure that this communication is performed in a timely and efficient manner.
  
    key_requirements:
      - Designate a single point of contact to enable recipients of the service to communicate directly and rapidly by electronic means
      - Make the contact information publicly available, easily accessible, user-friendly, and kept up to date
      - Allow recipients to choose a means of communication that does not solely rely on automated tools
      - Allocate sufficient human and financial resources to ensure communication is timely and efficient

    edge_cases:
      - If using chatbots or instant messaging, the provider must explicitly indicate when a recipient is communicating with an automated tool rather than a human
      - The prohibition on solely automated tools implies a mandatory "escape hatch" to human interaction or a non-automated channel, preventing providers from trapping users in endless bot loops
      - This obligation is distinct from Article 11 (authorities); while the contact points can be coordinated, the Article 12 channel has specific requirements for user-friendliness and accessibility that do not apply to the authority channel

  - article: 13
    title: Legal representatives
    context: |
      Recital 44 - Providers of intermediary services that are established in a third country and that offer services in the Union should designate a sufficiently mandated legal representative in the Union and provide information relating to their legal representatives to the relevant authorities and make it publicly available. In order to comply with that obligation, such providers of intermediary services should ensure that the designated legal representative has the necessary powers and resources to cooperate with the relevant authorities. This could be the case, for example, where a provider of intermediary services appoints a subsidiary undertaking of the same group as the provider, or its parent undertaking, if that subsidiary or parent undertaking is established in the Union. However, it might not be the case, for instance, when the legal representative is subject to reconstruction proceedings, bankruptcy, or personal or corporate insolvency. That obligation should allow for the effective oversight and, where necessary, enforcement of this Regulation in relation to those providers. It should be possible for a legal representative to be mandated, in accordance with national law, by more than one provider of intermediary services. It should be possible for the legal representative to also function as a point of contact, provided the relevant requirements of this Regulation are complied with.
    
    key_requirements:
      - Providers not established in the Union but offering services there must designate a legal or natural person as their legal representative in a Member State where they offer services
      - Mandate the legal representative to be addressed by authorities in addition to or instead of the provider on all compliance issues
      - Ensure the legal representative has the necessary powers and sufficient resources to cooperate efficiently with authorities and comply with decisions
      - Notify the name and contact details of the representative to the Digital Services Coordinator and make this information publicly available and up to date

    edge_cases:
      - Designating a representative does not constitute having an 'establishment' in the Union, preserving the provider's status as a non-EU entity for other legal contexts
      - A single legal representative can be mandated by more than one provider, and they can also function as the Article 11 Point of Contact if they meet all requirements

  - article: 14
    title: Terms of service
    context: |
      (45) "Whilst the freedom of contract of providers of intermediary services should in principle be respected, it is appropriate to set certain rules on the content, application and enforcement of the terms and conditions of those providers in the interests of transparency, the protection of recipients of the service and the avoidance of unfair or arbitrary outcomes. Providers of the intermediary services should clearly indicate and maintain up-to-date in their terms and conditions the information as to the grounds on the basis of which they may restrict the provision of their services. In particular, they should include information on any policies, procedures, measures and tools used for the purpose of content moderation, including algorithmic decision-making and human review, as well as the rules of procedure of their internal complaint-handling system. They should also provide easily accessible information on the right to terminate the use of the service. Providers of intermediary services may use graphical elements in their terms of service, such as icons or images, to illustrate the main elements of the information requirements set out in this Regulation. Providers should inform recipients of their service through appropriate means of significant changes made to terms and conditions, for instance when they modify the rules on information that is permitted on their service, or other such changes which could directly impact the ability of the recipients to make use of the service."
      (46) "Providers of intermediary services that are primarily directed at minors, for example through the design or marketing of the service, or which are used predominantly by minors, should make particular efforts to render the explanation of their terms and conditions easily understandable to minors."
      (47) "When designing, applying and enforcing those restrictions, providers of intermediary services should act in a non-arbitrary and non-discriminatory manner and take into account the rights and legitimate interests of the recipients of the service, including fundamental rights as enshrined in the Charter. For example, providers of very large online platforms should in particular pay due regard to freedom of expression and of information, including media freedom and pluralism. All providers of intermediary services should also pay due regard to relevant international standards for the protection of human rights, such as the United Nations Guiding Principles on Business and Human Rights."
      (48) "Given their special role and reach, it is appropriate to impose on very large online platforms and very large online search engines additional requirements regarding information and transparency of their terms and conditions. Consequently, providers of very large online platforms and very large online search engines should provide their terms and conditions in the official languages of all Member States in which they offer their services and should also provide recipients of the services with a concise and easily readable summary of the main elements of the terms and conditions. Such summaries should identify the main elements of the information requirements, including the possibility of easily opting out from optional clauses."
    key_requirements:
      - Include information on restrictions, policies, procedures, measures, and tools used for content moderation, including algorithmic decision-making and human review, as well as rules of procedure for the internal complaint handling system
      - Set out terms in clear, plain, intelligible, user-friendly, and unambiguous language, and make them publicly available in an easily accessible and machine-readable format
      - Inform recipients of any significant change to the terms and conditions
      - Act in a diligent, objective, and proportionate manner when applying and enforcing restrictions, with due regard to the rights and legitimate interests of all parties, including fundamental rights
      - Explain conditions and restrictions in a way minors can understand if the service is primarily directed at or predominantly used by them
      - For VLOPs and VLOSEs, provide a concise, easily accessible, and machine-readable summary of terms and conditions, including remedies and redress mechanisms
      - For VLOPs and VLOSEs, publish terms in the official languages of all Member States where services are offered

    edge_cases:
      - The obligation to disclose 'algorithmic decision-making' requires transparency about the existence and role of automated tools in moderation, distinguishing between fully automated decisions and those with human review
      - Article 14(4) effectively introduces a horizontal application of fundamental rights into private contract enforcement, meaning platforms cannot arbitrarily enforce terms if it disproportionately infringes on rights like freedom of expression
      - The requirement for 'machine-readable' formats implies that terms should not be locked in non-parsable formats (like image-only PDFs), facilitating automated analysis by regulators and researchers
  
  - article: 15
    title: Transparency reporting obligations
    context: |
      (49) "To ensure an adequate level of transparency and accountability, providers of intermediary services should make publicly available an annual report in a machine-readable format, in accordance with the harmonised requirements contained in this Regulation, on the content moderation in which they engage, including the measures taken as a result of the application and enforcement of their terms and conditions. However, in order to avoid disproportionate burdens, those transparency reporting obligations should not apply to providers that are micro or small enterprises as defined in Commission Recommendation 2003/361/EC (25) and which are not very large online platforms within the meaning of this Regulation."     
      (50) "Providers of hosting services play a particularly important role in tackling illegal content online, as they store information provided by and at the request of the recipients of the service and typically give other recipients access thereto, sometimes on a large scale. It is important that all providers of hosting services, regardless of their size, put in place easily accessible and user-friendly notice and action mechanisms that facilitate the notification of specific items of information that the notifying party considers to be illegal content to the provider of hosting services concerned (‘notice’), pursuant to which that provider can decide whether or not it agrees with that assessment and wishes to remove or disable access to that content (‘action’). Such mechanisms should be clearly identifiable, located close to the information in question and at least as easy to find and use as notification mechanisms for content that violates the terms and conditions of the hosting service provider. Provided the requirements on notices are met, it should be possible for individuals or entities to notify multiple specific items of allegedly illegal content through a single notice in order to ensure the effective operation of notice and action mechanisms. The notification mechanism should allow, but not require, the identification of the individual or the entity submitting a notice. For some types of items of information notified, the identity of the individual or the entity submitting a notice might be necessary to determine whether the information in question constitutes illegal content, as alleged. The obligation to put in place notice and action mechanisms should apply, for instance, to file storage and sharing services, web hosting services, advertising servers and paste bins, in so far as they qualify as hosting services covered by this Regulation."

    key_requirements:
      - Make clear, easily comprehensible reports on content moderation publicly available in a machine-readable format at least once a year
      - Include statistics on orders received from Member States authorities, categorized by illegal content type and median time to give effect
      - For hosting services, report on notices submitted under Article 16, including the number of notices, type of content, actions taken, and use of automated means
      - Report on own-initiative moderation, including the use of automated tools, training provided to staff, and the number and type of restrictions applied
      - Include data on the functioning of the internal complaint-handling system, including the number of complaints, decisions taken, reversals, and median time
      - Provide qualitative descriptions and accuracy/error rate indicators for any automated means used for content moderation

    edge_cases:
      - This obligation does not apply to providers qualifying as micro or small enterprises (MSEs), unless they are designated as VLOPs
      - The requirement for a 'machine-readable format' means standard PDFs may not be sufficient; data must be structured for software processing (e.g., CSV, JSON)
      - There is a distinction in scope. Statistics on 'notices' (Article 16) are required only for hosting services, whereas statistics on 'orders' (Articles 9 and 10) apply to all intermediary services

  # ============================================================================
  # Section 2: Hosting Service Obligations (Articles 16-18)
  # Apply to: Hosting, Online Platform, Online Marketplace, Search Engine
  # ============================================================================

  - article: 16
    title: Notice and action mechanisms
    context: |
      (50) "Providers of hosting services play a particularly important role in tackling illegal content online, as they store information provided by and at the request of the recipients of the service and typically give other recipients access thereto, sometimes on a large scale. It is important that all providers of hosting services, regardless of their size, put in place easily accessible and user-friendly notice and action mechanisms that facilitate the notification of specific items of information that the notifying party considers to be illegal content to the provider of hosting services concerned (‘notice’), pursuant to which that provider can decide whether or not it agrees with that assessment and wishes to remove or disable access to that content (‘action’). Such mechanisms should be clearly identifiable, located close to the information in question and at least as easy to find and use as notification mechanisms for content that violates the terms and conditions of the hosting service provider. Provided the requirements on notices are met, it should be possible for individuals or entities to notify multiple specific items of allegedly illegal content through a single notice in order to ensure the effective operation of notice and action mechanisms. The notification mechanism should allow, but not require, the identification of the individual or the entity submitting a notice. For some types of items of information notified, the identity of the individual or the entity submitting a notice might be necessary to determine whether the information in question constitutes illegal content, as alleged. The obligation to put in place notice and action mechanisms should apply, for instance, to file storage and sharing services, web hosting services, advertising servers and paste bins, in so far as they qualify as hosting services covered by this Regulation."
      (51) "Having regard to the need to take due account of the fundamental rights guaranteed under the Charter of all parties concerned, any action taken by a provider of hosting services pursuant to receiving a notice should be strictly targeted, in the sense that it should serve to remove or disable access to the specific items of information considered to constitute illegal content, without unduly affecting the freedom of expression and of information of recipients of the service. Notices should therefore, as a general rule, be directed to the providers of hosting services that can reasonably be expected to have the technical and operational ability to act against such specific items. The providers of hosting services who receive a notice for which they cannot, for technical or operational reasons, remove the specific item of information should inform the person or entity who submitted the notice."
      (52) "The rules on such notice and action mechanisms should be harmonised at Union level, so as to provide for the timely, diligent and non-arbitrary processing of notices on the basis of rules that are uniform, transparent and clear and that provide for robust safeguards to protect the right and legitimate interests of all affected parties, in particular their fundamental rights guaranteed by the Charter, irrespective of the Member State in which those parties are established or reside and of the field of law at issue. Those fundamental rights include but are not limited to, for the recipients of the service, the right to freedom of expression and of information, the right to respect for private and family life, the right to protection of personal data, the right to non-discrimination and the right to an effective remedy; for the service providers, the freedom to conduct a business, including the freedom of contract; for parties affected by illegal content, the right to human dignity, the rights of the child, the right to protection of property, including intellectual property, and the right to non-discrimination. Providers of hosting services should act upon notices in a timely manner, in particular by taking into account the type of illegal content being notified and the urgency of taking action. For instance, such providers can be expected to act without delay when allegedly illegal content involving a threat to life or safety of persons is being notified. The provider of hosting services should inform the individual or entity notifying the specific content without undue delay after taking a decision whether or not to act upon the notice."
      (53) The notice and action mechanisms should allow for the submission of notices which are sufficiently precise and adequately substantiated to enable the provider of hosting services concerned to take an informed and diligent decision, compatible with the freedom of expression and of information, in respect of the content to which the notice relates, in particular whether or not that content is to be considered illegal content and is to be removed or access thereto is to be disabled. Those mechanisms should be such as to facilitate the provision of notices that contain an explanation of the reasons why the individual or the entity submitting a notice considers that content to be illegal content, and a clear indication of the location of that content. Where a notice contains sufficient information to enable a diligent provider of hosting services to identify, without a detailed legal examination, that it is clear that the content is illegal, the notice should be considered to give rise to actual knowledge or awareness of illegality. Except for the submission of notices relating to offences referred to in Articles 3 to 7 of Directive 2011/93/EU of the European Parliament and of the Council (26), those mechanisms should ask the individual or the entity submitting a notice to disclose its identity in order to avoid misuse."    

    key_requirements:
      - Put in place mechanisms to allow any individual or entity to notify specific items of illegal content
      - Ensure mechanisms are easy to access, user-friendly, and allow submission exclusively by electronic means
      - Facilitate the submission of notices containing an explanation of reasons, clear indication of location (URLs), name and email of the submitter, and a bona fide belief statement
      - Send a confirmation of receipt to the submitter without undue delay
      - Notify the submitter of the decision taken and available redress possibilities without undue delay
      - Process notices in a timely, diligent, non-arbitrary, and objective manner

    edge_cases:
      - The requirement to collect the submitter's name and email does not apply if the content involves offences related to child sexual abuse under Directive 2011/93/EU
      - A sufficiently precise notice creates actual knowledge of illegality, which can remove the hosting provider's liability exemption under Article 6 if they fail to act expeditiously
      - If automated means are used for processing or decision-making, this fact must be disclosed in the notification sent to the submitter

  - article: 17
    title: Statement of reasons
    context: |
      (54) "Where a provider of hosting services decides, on the ground that the information provided by the recipients is illegal content or is incompatible with its terms and conditions, to remove or disable access to information provided by a recipient of the service or to otherwise restrict its visibility or monetisation, for instance following receipt of a notice or acting on its own initiative, including exclusively by automated means, that provider should inform in a clear and easily comprehensible way the recipient of its decision, the reasons for its decision and the available possibilities for redress to contest the decision, in view of the negative consequences that such decisions may have for the recipient, including as regards the exercise of its fundamental right to freedom of expression. That obligation should apply irrespective of the reasons for the decision, in particular whether the action has been taken because the information notified is considered to be illegal content or incompatible with the applicable terms and conditions. Where the decision was taken following receipt of a notice, the provider of hosting services should only reveal the identity of the person or entity who submitted the notice to the recipient of the service where this information is necessary to identify the illegality of the content, such as in cases of infringements of intellectual property rights."
      (55) "Restriction of visibility may consist in demotion in ranking or in recommender systems, as well as in limiting accessibility by one or more recipients of the service or blocking the user from an online community without the user being aware (‘shadow banning’). The monetisation via advertising revenue of information provided by the recipient of the service can be restricted by suspending or terminating the monetary payment or revenue associated to that information. The obligation to provide a statement of reasons should however not apply with respect to deceptive high-volume commercial content disseminated through intentional manipulation of the service, in particular inauthentic use of the service such as the use of bots or fake accounts or other deceptive uses of the service. Irrespective of other possibilities to challenge the decision of the provider of hosting services, the recipient of the service should always have a right to effective remedy before a court in accordance with the national law."
    key_requirements:
      - Provide a clear and specific statement of reasons to any affected recipient for restrictions on content visibility, monetization, service provision, or account status
      - Deliver the statement at the latest when the restriction is imposed, regardless of the ground for the decision
      - Include specific information. Facts and circumstances, use of automated means, the legal or contractual ground relied upon, and explanations of why the content is considered illegal or incompatible
      - Provide clear, user-friendly information on available redress possibilities, including internal complaint-handling, out-of-court dispute settlement, and judicial redress
      - Ensure the information is precise enough to allow the recipient to effectively exercise their right to redress

    edge_cases:
      - The obligation to provide a statement of reasons does not apply to 'deceptive high-volume commercial content' (e.g., spam bots), preventing systems from being overwhelmed by bad actors
      - Restrictions on 'visibility' (often called shadow-banning or demotion) trigger this obligation just as removal does; the user must be informed if their reach is artificially suppressed
      - The identity of a notifier should only be revealed to the affected user where strictly necessary to identify the illegality (e.g., in intellectual property infringements), otherwise, it remains confidential

  - article: 18
    title: Notification of suspicions of criminal offences
    context: |
      Recital 56 – Obligation to Report Criminal Offences:"A provider of hosting services may in some instances become aware, such as through a notice by a notifying party or through its own voluntary measures, of information relating to certain activity of a recipient of the service, such as the provision of certain types of illegal content, that reasonably justify, having regard to all relevant circumstances of which the provider of hosting services is aware, the suspicion that that recipient may have committed, may be committing or is likely to commit a criminal offence involving a threat to the life or safety of person or persons, such as offences specified in Directive 2011/36/EU of the European Parliament and of the Council, Directive 2011/93/EU or Directive (EU) 2017/541 of the European Parliament and of the Council. For example, specific items of content could give rise to a suspicion of a threat to the public, such as incitement to terrorism within the meaning of Article 21 of Directive (EU) 2017/541. In such instances, the provider of hosting services should inform without delay the competent law enforcement authorities of such suspicion. The provider of hosting services should provide all relevant information available to it, including, where relevant, the content in question and, if available, the time when the content was published, including the designated time zone, an explanation of its suspicion and the information necessary to locate and identify the relevant recipient of the service. This Regulation does not provide the legal basis for profiling of recipients of the services with a view to the possible identification of criminal offences by providers of hosting services. Providers of hosting services should also respect other applicable rules of Union or national law for the protection of the rights and freedoms of individuals when informing law enforcement authorities.
    
    key_requirements:
      - Promptly inform law enforcement or judicial authorities if the provider becomes aware of information giving rise to a suspicion of a criminal offence involving a threat to the life or safety of a person
      - Provide all relevant information available, including the content, time of publication, explanation of the suspicion, and information to locate the recipient
      - If the Member State concerned cannot be identified, inform the authorities of the Member State of establishment/residence or Europol
      - Establish internal procedures to ensure such suspicions are identified and reported without delay upon becoming aware of them

    edge_cases:
      - This obligation is strictly limited to offences involving a 'threat to the life or safety' of persons (e.g., terrorism, child sexual abuse, trafficking); it does not mandate reporting for non-life-threatening crimes like copyright infringement or fraud
      - Article 18 does not provide a legal basis for the profiling of recipients; providers should not use this obligation to justify general monitoring or automated profiling of users to find crimes
      - The notification must respect other applicable rules on the protection of rights and freedoms, implying a balance with data protection laws (GDPR) when sharing user data with law enforcement

  # ============================================================================
  # Section 3: Online Platform Obligations (Articles 19-28)
  # Apply to: Online Platform, Online Marketplace, Search Engine
  # NOTE: Articles 20-28 have SME exemption (unless marketplace or VLOP/VLOSE)
  # ============================================================================

  - article: 19
    title: SME Exemption for Platform Obligations
    context: |
     Article 2 Commission Recommendation 2003/361/EC - The category of micro, small and medium-sized enterprises (SMEs) is made up of enterprises which employ fewer than 250 persons and which have an annual turnover not exceeding €50 million, and/or an annual balance sheet total not exceeding €43 million. Within the SME category, a small enterprise is defined as an enterprise which employs fewer than 50 persons and whose annual turnover and/or annual balance sheet total does not exceed €10 million. Within the SME category, a micro enterprise is defined as an enterprise which employs fewer than 10 persons and whose annual turnover and/or annual balance sheet total does not exceed €2 million.”
     (69) "To avoid disproportionate burdens, the additional obligations imposed under this Regulation on providers of online platforms, including platforms allowing consumers to conclude distance contracts with traders, should not apply to providers that qualify as micro or small enterprises as defined in Recommendation 2003/361/EC. For the same reason, those additional obligations should also not apply to providers of online platforms that previously qualified as micro or small enterprises during a period of 12 months after they lose that status. Such providers should not be excluded from the obligation to provide information on the average monthly active recipients of the service at the request of the Digital Services Coordinator of establishment or the Commission. However, considering that very large online platforms or very large online search engines have a larger reach and a greater impact in influencing how recipients of the service obtain information and communicate online, such providers should not benefit from that exclusion, irrespective of whether they qualify or recently qualified as micro or small enterprises. The consolidation rules laid down in Recommendation 2003/361/EC help ensure that any circumvention of those additional obligations is prevented. Nothing in this Regulation precludes providers of online platforms that are covered by that exclusion from setting up, on a voluntary basis, a system that complies with one or more of those obligations."

    key_requirements:
      - Exempts providers qualifying as micro or small enterprises (MSEs) from the obligations laid down in Section 3 (Articles 20 to 28)
      - Definition of MSE. Enterprises employing fewer than 50 persons and whose annual turnover or annual balance sheet total does not exceed EUR 10 million
      - The exemption continues to apply for a period of 12 months after the provider loses the status of a micro or small enterprise
      - The exemption does NOT apply if the provider is designated as a Very Large Online Platform (VLOP) or Very Large Online Search Engine (VLOSE), regardless of size
      - MSEs must still comply with the obligation to provide information on average monthly active recipients (AMAR) if requested by the Digital Services Coordinator or Commission (Article 24(3))

    edge_cases:
    - Correction on Marketplaces. Contrary to the initial assumption, Article 29 explicitly exempts MSEs from Section 4 obligations (Articles 30-32 regarding trader traceability and compliance by design); small marketplaces are therefore exempt from these specific checks
    - The Consolidation Trap. MSE status is assessed according to Commission Recommendation 2003/361/EC, which includes rules on 'partner' and 'linked' enterprises; a small subsidiary of a large corporate group does not qualify for the exemption
    - Voluntary Compliance. The Regulation explicitly states that exempt providers are not precluded from setting up systems that comply with these obligations voluntarily (e.g., establishing an internal complaint-handling system despite being small)

  - article: 20
    title: Internal complaint-handling system
    context: |
      (58) Recipients of the service should be able to easily and effectively contest certain decisions of providers of online platforms concerning the illegality of content or its incompatibility with the terms and conditions that negatively affect them. Therefore, providers of online platforms should be required to provide for internal complaint-handling systems, which meet certain conditions that aim to ensure that the systems are easily accessible and lead to swift, non-discriminatory, non-arbitrary and fair outcomes, and are subject to human review where automated means are used. Such systems should enable all recipients of the service to lodge a complaint and should not set formal requirements, such as referral to specific, relevant legal provisions or elaborate legal explanations. Recipients of the service who submitted a notice through the notice and action mechanism provided for in this Regulation or through the notification mechanism for content that violate the terms and conditions of the provider of online platforms should be entitled to use the complaint mechanism to contest the decision of the provider of online platforms on their notices, including when they consider that the action taken by that provider was not adequate. The possibility to lodge a complaint for the reversal of the contested decisions should be available for at least six months, to be calculated from the moment at which the provider of online platforms informed the recipient of the service of the decision.
      (59) In addition, provision should be made for the possibility of engaging, in good faith, in the out-of-court dispute settlement of such disputes, including those that could not be resolved in a satisfactory manner through the internal complaint-handling systems, by certified bodies that have the requisite independence, means and expertise to carry out their activities in a fair, swift and cost-effective manner. The independence of the out-of-court dispute settlement bodies should be ensured also at the level of the natural persons in charge of resolving disputes, including through rules on conflict of interest. The fees charged by the out-of-court dispute settlement bodies should be reasonable, accessible, attractive, inexpensive for consumers and proportionate, and assessed on a case-by-case basis. Where an out-of-court dispute settlement body is certified by the competent Digital Services Coordinator, that certification should be valid in all Member States. Providers of online platforms should be able to refuse to engage in out-of-court dispute settlement procedures under this Regulation when the same dispute, in particular as regards the information concerned and the grounds for taking the contested decision, the effects of the decision and the grounds raised for contesting the decision, has already been resolved by or is already subject to an ongoing procedure before the competent court or before another competent out-of-court dispute settlement body. Recipients of the service should be able to choose between the internal complaint mechanism, an out-of-court dispute settlement and the possibility to initiate, at any stage, judicial proceedings. Since the outcome of the out-of-court dispute settlement procedure is not binding, the parties should not be prevented from initiating judicial proceedings in relation to the same dispute. The possibilities to contest decisions of providers of online platforms thus created should leave unaffected in all respects the possibility to seek judicial redress in accordance with the laws of the Member State concerned, and therefore should not affect the exercise of the right to an effective judicial remedy under Article 47 of the Charter. The provisions in this Regulation on out-of-court dispute settlement should not require Member States to establish such out-of-court settlement bodies.
      (45) "Whilst the freedom of contract of providers of intermediary services should in principle be respected, it is appropriate to set certain rules on the content, application and enforcement of the terms and conditions of those providers in the interests of transparency, the protection of recipients of the service and the avoidance of unfair or arbitrary outcomes. Providers of the intermediary services should clearly indicate and maintain up-to-date in their terms and conditions the information as to the grounds on the basis of which they may restrict the provision of their services. In particular, they should include information on any policies, procedures, measures and tools used for the purpose of content moderation, including algorithmic decision-making and human review, as well as the rules of procedure of their internal complaint-handling system. They should also provide easily accessible information on the right to terminate the use of the service. Providers of intermediary services may use graphical elements in their terms of service, such as icons or images, to illustrate the main elements of the information requirements set out in this Regulation. Providers should inform recipients of their service through appropriate means of significant changes made to terms and conditions, for instance when they modify the rules on information that is permitted on their service, or other such changes which could directly impact the ability of the recipients to make use of the service."

    key_requirements:
      - Provide recipients of the service, including individuals or entities that have submitted a notice, with access to an effective internal complaint-handling system for at least six months following a moderation decision
      - Ensure the system allows complaints against decisions to remove content, disable access, restrict visibility, suspend or terminate services or accounts, or restrict monetization
      - The system must be easy to access, user-friendly, and enable the submission of complaints electronically and free of charge
      - Handle complaints in a timely, non-discriminatory, diligent, and non-arbitrary manner
      - Ensure that decisions on complaints are taken under the supervision of appropriately qualified staff and not solely on the basis of automated means
      - Inform the complainant without undue delay of the reasoned decision and the possibility of out-of-court dispute settlement and other redress options

    edge_cases:
      - The right to lodge a complaint extends to the notifier (the person who reported the content) if the platform decides not to take action, not just the user whose content was removed
      - The requirement for human supervision (Article 20(6)) effectively prohibits fully automated appeals processes; a human must review the decision if contested
      - The system must enable complaints without imposing formal requirements, such as requiring the user to cite specific legal provisions or provide elaborate legal explanations (Recital 58)

  - article: 21
    title: Out-of-court dispute settlement
    context: |
      (59) In addition, provision should be made for the possibility of engaging, in good faith, in the out-of-court dispute settlement of such disputes, including those that could not be resolved in a satisfactory manner through the internal complaint-handling systems, by certified bodies that have the requisite independence, means and expertise to carry out their activities in a fair, swift and cost-effective manner. The independence of the out-of-court dispute settlement bodies should be ensured also at the level of the natural persons in charge of resolving disputes, including through rules on conflict of interest. The fees charged by the out-of-court dispute settlement bodies should be reasonable, accessible, attractive, inexpensive for consumers and proportionate, and assessed on a case-by-case basis. Where an out-of-court dispute settlement body is certified by the competent Digital Services Coordinator, that certification should be valid in all Member States. Providers of online platforms should be able to refuse to engage in out-of-court dispute settlement procedures under this Regulation when the same dispute, in particular as regards the information concerned and the grounds for taking the contested decision, the effects of the decision and the grounds raised for contesting the decision, has already been resolved by or is already subject to an ongoing procedure before the competent court or before another competent out-of-court dispute settlement body. Recipients of the service should be able to choose between the internal complaint mechanism, an out-of-court dispute settlement and the possibility to initiate, at any stage, judicial proceedings. Since the outcome of the out-of-court dispute settlement procedure is not binding, the parties should not be prevented from initiating judicial proceedings in relation to the same dispute. The possibilities to contest decisions of providers of online platforms thus created should leave unaffected in all respects the possibility to seek judicial redress in accordance with the laws of the Member State concerned, and therefore should not affect the exercise of the right to an effective judicial remedy under Article 47 of the Charter. The provisions in this Regulation on out-of-court dispute settlement should not require Member States to establish such out-of-court settlement bodies.
      (58) "Recipients of the service should be able to easily and effectively contest certain decisions of providers of online platforms concerning the illegality of content or its incompatibility with the terms and conditions that negatively affect them. Therefore, providers of online platforms should be required to provide for internal complaint-handling systems, which meet certain conditions that aim to ensure that the systems are easily accessible and lead to swift, non-discriminatory, non-arbitrary and fair outcomes, and are subject to human review where automated means are used. Such systems should enable all recipients of the service to lodge a complaint and should not set formal requirements, such as referral to specific, relevant legal provisions or elaborate legal explanations. Recipients of the service who submitted a notice through the notice and action mechanism provided for in this Regulation or through the notification mechanism for content that violate the terms and conditions of the provider of online platforms should be entitled to use the complaint mechanism to contest the decision of the provider of online platforms on their notices, including when they consider that the action taken by that provider was not adequate. The possibility to lodge a complaint for the reversal of the contested decisions should be available for at least six months, to be calculated from the moment at which the provider of online platforms informed the recipient of the service of the decision."
      (60) For contractual consumer-to-business disputes regarding the purchase of goods or services, Directive 2013/11/EU ensures that Union consumers and businesses in the Union have access to quality-certified alternative dispute resolution entities. In this regard, it should be clarified that the rules of this Regulation on out-of-court dispute settlement are without prejudice to that Directive, including the right of consumers under that Directive to withdraw from the procedure at any stage if they are dissatisfied with the performance or the operation of the procedure.
    key_requirements:
      - Recipients of the service (including notifiers) are entitled to select a certified out-of-court dispute settlement body to resolve disputes relating to decisions referred to in Article 20(1)
      - Providers of online platforms must ensure information about this possibility is accessible, clear, and user-friendly on their online interface
      - Both parties must engage in good faith with the selected body to resolve the dispute
      - If the body decides in favor of the recipient, the platform must bear all fees and reimburse the recipient's reasonable expenses
      - If the body decides in favor of the platform, the recipient is not required to reimburse the platform's costs, unless the recipient manifestly acted in bad faith
      - The dispute settlement bodies must be certified by the Digital Services Coordinator, ensuring independence, expertise, and fair rules of procedure

    edge_cases:
      - Providers can refuse to engage if the same dispute regarding the same information and grounds has already been resolved by or is pending before a court or another certified body
      - The outcome of the settlement is not legally binding, meaning parties are not prevented from initiating judicial proceedings at any stage
      - This mechanism applies to all recipients of the service, which includes both consumers and professional business users, but is without prejudice to consumer-specific ADR rules under Directive 2013/11/EU
 

  - article: 22
    title: Trusted flaggers
    context: |
      (61) Action against illegal content can be taken more quickly and reliably where providers of online platforms take the necessary measures to ensure that notices submitted by trusted flaggers, acting within their designated area of expertise, through the notice and action mechanisms required by this Regulation are treated with priority, without prejudice to the requirement to process and decide upon all notices submitted under those mechanisms in a timely, diligent and non-arbitrary manner. Such trusted flagger status should be awarded by the Digital Services Coordinator of the Member State in which the applicant is established and should be recognised by all providers of online platforms within the scope of this Regulation. Such trusted flagger status should only be awarded to entities, and not individuals, that have demonstrated, among other things, that they have particular expertise and competence in tackling illegal content and that they work in a diligent, accurate and objective manner. Such entities can be public in nature, such as, for terrorist content, internet referral units of national law enforcement authorities or of the European Union Agency for Law Enforcement Cooperation (Europol) or they can be non-governmental organisations and private or semi-public bodies such as the organisations part of the INHOPE network of hotlines for reporting child sexual abuse material and organisations committed to notifying illegal racist and xenophobic expressions online. To avoid diminishing the added value of such mechanism, the overall number of trusted flaggers awarded in accordance with this Regulation should be limited. In particular, industry associations representing their members' interests are encouraged to apply for the status of trusted flaggers, without prejudice to the right of private entities or individuals to enter into bilateral agreements with the providers of online platforms.
      (62) Trusted flaggers should publish easily comprehensible and detailed reports on notices submitted in accordance with this Regulation. Those reports should indicate information such as the number of notices categorised by the provider of hosting services, the type of content, and the action taken by the provider. Given that trusted flaggers have demonstrated expertise and competence, the processing of notices submitted by trusted flaggers can be expected to be less burdensome and therefore faster compared to notices submitted by other recipients of the service. However, the average time taken to process may still vary depending on factors including the type of illegal content, the quality of notices, and the actual technical procedures put in place for the submission of such notices. For example, while the Code of conduct on countering illegal hate speech online of 2016 sets a benchmark for the participating companies with respect to the time needed to process valid notifications for removal of illegal hate speech, other types of illegal content may take considerably different timelines for processing, depending on the specific facts and circumstances and types of illegal content at stake. In order to avoid abuses of the trusted flagger status, it should be possible to suspend such status when a Digital Services Coordinator of establishment opened an investigation based on legitimate reasons. The rules of this Regulation on trusted flaggers should not be understood to prevent providers of online platforms from giving similar treatment to notices submitted by entities or individuals that have not been awarded trusted flagger status under this Regulation, from otherwise cooperating with other entities, in accordance with the applicable law, including this Regulation and Regulation (EU) 2016/794 of the European Parliament and of the Council (29). The rules of this Regulation should not prevent the providers of online platforms from making use of such trusted flagger or similar mechanisms to take quick and reliable action against content that is incompatible with their terms and conditions, in particular against content that is harmful for vulnerable recipients of the service, such as minors.
    key_requirements:
      - Take necessary technical and organizational measures to ensure notices submitted by trusted flaggers are treated with priority
      - Process and decide upon notices from trusted flaggers without undue delay
      - Recognize the status of 'trusted flagger' awarded by the Digital Services Coordinator of any Member State
      - Cooperate with trusted flaggers, who are required to publish annual reports on their notices
      - Communicate information to the Digital Services Coordinator if there is evidence that a trusted flagger submitted a significant number of inaccurate or unsubstantiated notices, to trigger potential suspension of their status

    edge_cases:
      - Trusted flagger status is awarded only to entities (not individuals) that have demonstrated specific expertise and independence from online platforms
      - While trusted flagger notices get priority, the platform must still process them with diligence and non-arbitrariness; priority does not mean automatic removal without review
      - Platforms can maintain bilateral agreements with other entities or individuals that are not designated trusted flaggers, but the statutory obligation for priority processing applies specifically to the designated ones

  - article: 23
    title: Measures and protection against misuse
    context: |
      (63) The misuse of online platforms by frequently providing manifestly illegal content or by frequently submitting manifestly unfounded notices or complaints under the mechanisms and systems, respectively, established under this Regulation undermines trust and harms the rights and legitimate interests of the parties concerned. Therefore, there is a need to put in place appropriate, proportionate and effective safeguards against such misuse, that need to respect the rights and legitimate interests of all parties involved, including the applicable fundamental rights and freedoms as enshrined in the Charter, in particular the freedom of expression. Information should be considered to be manifestly illegal content and notices or complaints should be considered manifestly unfounded where it is evident to a layperson, without any substantive analysis, that the content is illegal or, respectively, that the notices or complaints are unfounded.
      (64) Under certain conditions, providers of online platforms should temporarily suspend their relevant activities in respect of the person engaged in abusive behaviour. This is without prejudice to the freedom by providers of online platforms to determine their terms and conditions and establish stricter measures in the case of manifestly illegal content related to serious crimes, such as child sexual abuse material. For reasons of transparency, this possibility should be set out, clearly and in sufficient detail, in the terms and conditions of the online platforms. Redress should always be open to the decisions taken in this regard by providers of online platforms and they should be subject to oversight by the competent Digital Services Coordinator. Providers of online platforms should send a prior warning before deciding on the suspension, which should include the reasons for the possible suspension and the means of redress against the decision of the providers of the online platform. When deciding on the suspension, providers of online platforms should send the statement of reasons in accordance with the rules set out in this Regulation. The rules of this Regulation on misuse should not prevent providers of online platforms from taking other measures to address the provision of illegal content by recipients of their service or other misuse of their services, including through the violation of their terms and conditions, in accordance with the applicable Union and national law. Those rules are without prejudice to any possibility to hold the persons engaged in misuse liable, including for damages, provided for in Union or national law.
    key_requirements:
      - Suspend the provision of services, for a reasonable period and after a prior warning, to recipients who frequently provide manifestly illegal content
      - Suspend the processing of notices and complaints, for a reasonable period and after a prior warning, from individuals or entities that frequently submit manifestly unfounded notices or complaints
      - Clearly detail the policy regarding misuse in terms and conditions, including examples of facts and circumstances taken into account when assessing misuse and the duration of suspension
      - Assess misuse on a case-by-case basis using objective criteria. Absolute numbers of items/notices, relative proportion to total activity, gravity of the misuse, and intention (where identifiable)
      - Issue a statement of reasons for the suspension and ensure redress mechanisms are available
    edge_cases:
      - "'Manifestly illegal' or 'manifestly unfounded' is defined as evident to a layperson without substantive analysis; if a detailed legal assessment is required to determine illegality, this specific suspension obligation does not apply"
      - "This article creates a statutory baseline but is without prejudice to a platform's freedom to set stricter voluntary measures for serious crimes (e.g., immediate termination for Child Sexual Abuse Material without prior warning)"
      - "Suspensions regarding unfounded notices must be handled carefully to avoid discouraging legitimate reporting; the threshold is 'frequently' and 'manifestly unfounded'"

  - article: 24
    title: Transparency reporting (enhanced for platforms)
    context: |
      (65) In addition to the transparency reporting obligations applicable to providers of intermediary services, providers of online platforms should publish information on the outcomes of the internal complaint-handling systems and the out-of-court dispute settlement referred to in this Regulation. They should also publish information on the number of suspensions carried out under this Regulation in respect of recipients of the service who frequently provide manifestly illegal content or who frequently submit manifestly unfounded notices or complaints. To enable detection and designation of very large online platforms and very large online search engines, providers of online platforms or of online search engines should be required to make public the average monthly active recipients of the service in the Union. The average monthly active recipients of the service in the Union should be calculated on the basis of a six-month period, which should ensure that one-off spikes in usage do not trigger the obligations applicable to very large online platforms and of very large online search engines. It is also necessary to provide for a mechanism to confirm the accuracy of the data published in this regard and, where necessary, of the calculations to ensure compliance with this Regulation.
      (66) Providers of online platforms should systematically communicate, without undue delay, all the decisions and statements of reasons, as referred to in this Regulation, that they take in respect of information provided by the recipients of the service, to the Commission. That information should be submitted for inclusion in a publicly available database managed by the Commission. To avoid information that can identify individuals from being made available to the public, providers of online platforms should ensure that the information submitted does not contain any personal data. That information should be collected for the purposes of transparency and enforcement of this Regulation. It can, for instance, enable interested parties, such as researchers and civil society organisations, to scrutinise the enforcement of this Regulation and to understand the procedures and practices of the online platforms as regards content moderation. That information can also enable the Commission to better design additional tools or templates for reporting, or to provide technical guidance.
      (77) "Very large online platforms and very large online search engines may cause societal risks, different in scope and impact from those caused by smaller platforms. Providers of such very large online platforms and of very large online search engines should therefore bear the highest standard of due diligence obligations, proportionate to their societal impact. Once the number of active recipients of an online platform or of active recipients of an online search engine, calculated as an average over a period of six months, reaches a significant share of the Union population, the systemic risks the online platform or online search engine poses may have a disproportionate impact in the Union. Such significant reach should be considered to exist where such number exceeds an operational threshold set at 45 million, that is, a number equivalent to 10 % of the Union population. This operational threshold should be kept up to date and therefore the Commission should be empowered to supplement the provisions of this Regulation by adopting delegated acts, where necessary."
      (78) "In view of the network effects characterising the platform economy, the user base of an online platform or an online search engine may quickly expand and reach the dimension of a very large online platform or a very large online search engine, with the related impact on the internal market. This may be the case in the event of exponential growth experienced in short periods of time, or by a large global presence and turnover allowing the online platform or the online search engine to fully exploit network effects and economies of scale and of scope. A high annual turnover or market capitalisation can in particular be an indication of fast scalability in terms of user reach. In those cases, the Digital Services Coordinator of establishment or the Commission should be able to request more frequent reporting from the provider of the online platform or of the online search engine on the number of active recipients of the service to be able to timely identify the moment at which that platform or that search engine should be designated as a very large online platform or very large online search engine, respectively, for the purposes of this Regulation."
      (100) "In view of the additional risks relating to their activities and their additional obligations under this Regulation, additional transparency requirements should apply specifically to very large online platforms and very large online search engines, notably to report comprehensively on the risk assessments performed and subsequent measures adopted as provided by this Regulation."

    key_requirements:
      - In addition to Article 15 data, include statistics on out-of-court dispute settlements (outcomes, median time, share of implemented decisions)
      - Report the number of suspensions imposed for providing manifestly illegal content, submitting unfounded notices, or submitting unfounded complaints
      - Publish the number of average monthly active recipients (AMAR) in the Union on the online interface at least once every six months
      - Submit all statements of reasons (decisions to restrict content/accounts) to the Commission’s publicly accessible database without undue delay, ensuring no personal data is included
      - Communicate AMAR data and calculation methodology to the Digital Services Coordinator or Commission upon request

    edge_cases:
      - Unlike VLOPs (Article 42), standard online platforms are only required to publish the full transparency report annually, not every six months; only the AMAR figure must be updated semi-annually
      - Micro and Small Enterprises (MSEs) are exempt from most of this Article, EXCEPT for Article 24(3): they must disclose their AMAR to the regulator if requested, even though they don't have to publish it publicly
      - The obligation to submit data to the Commission's transparency database (Article 24(5)) is a continuous, near real-time obligation ('without undue delay'), distinct from the periodic annual reporting

  - article: "24.3"
    title: Information on average monthly active recipients (non-exempt for SMEs)
    context: |
      (77) "Very large online platforms and very large online search engines may cause societal risks, different in scope and impact from those caused by smaller platforms. Providers of such very large online platforms and of very large online search engines should therefore bear the highest standard of due diligence obligations, proportionate to their societal impact. Once the number of active recipients of an online platform or of active recipients of an online search engine, calculated as an average over a period of six months, reaches a significant share of the Union population, the systemic risks the online platform or online search engine poses may have a disproportionate impact in the Union. Such significant reach should be considered to exist where such number exceeds an operational threshold set at 45 million, that is, a number equivalent to 10 % of the Union population. This operational threshold should be kept up to date and therefore the Commission should be empowered to supplement the provisions of this Regulation by adopting delegated acts, where necessary."
      (78) "In view of the network effects characterising the platform economy, the user base of an online platform or an online search engine may quickly expand and reach the dimension of a very large online platform or a very large online search engine, with the related impact on the internal market. This may be the case in the event of exponential growth experienced in short periods of time, or by a large global presence and turnover allowing the online platform or the online search engine to fully exploit network effects and economies of scale and of scope. A high annual turnover or market capitalisation can in particular be an indication of fast scalability in terms of user reach. In those cases, the Digital Services Coordinator of establishment or the Commission should be able to request more frequent reporting from the provider of the online platform or of the online search engine on the number of active recipients of the service to be able to timely identify the moment at which that platform or that search engine should be designated as a very large online platform or very large online search engine, respectively, for the purposes of this Regulation."
    key_requirements:
      - Communicate the number of average monthly active recipients (AMAR) in the Union to the Digital Services Coordinator of establishment or the Commission upon their request
      - Provide this information without undue delay, updated to the moment of the request
      - If required by the authority, provide additional explanations and substantiation regarding the calculation methodology and data used
      - Ensure that the information provided does not contain personal data

    edge_cases:
      - This specific obligation is the *only* part of Section 3 (Online Platform obligations) that applies to Micro and Small Enterprises (MSEs); they are exempt from public reporting but must report to the regulator if asked
      - This mechanism is designed to monitor 'fast scalability' (Recital 78), allowing regulators to identify when a small platform is approaching the 45 million user threshold to become a VLOP, even if they are financially small
      - Failure to comply with this request prevents the Commission from verifying VLOP status but does not stop them from designating the provider as a VLOP based on other available information (Article 33(4))

  - article: 25
    title: Online interface design and organisation
    context: |
      (67) Providers of online platforms should neither use so-called dark patterns to deceive users nor directly or indirectly subvert or impair the autonomy, decision-making or choice of the recipients of the service, including by means of their online interface. Examples of such practices include making the cancellation of a service more difficult than signing up to it, or making certain choices more prominent than others through visual, auditory or other elements. Those practices can be particularly detrimental to minors and can be associated with mental health issues, and with marginalised groups, and can also be used to encourage harmful behaviour or content. In accordance with this Regulation, it should, nevertheless, be possible to recommend information as part of a specific information society service, also where the recommendation is carried out through ranking, rating, review or search functions, including ordering of information, also where it is prioritised or given more relative prominence by means of visual, auditory or other elements. That information should, in particular, be provided in an easily comprehensible and legible manner. Where such practices are already prohibited by applicable Union law such as Directive 2005/29/EC on unfair business-to-consumer commercial practices in the internal market or Regulation (EU) 2016/679 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data (General Data Protection Regulation), the rules set out in this Regulation in this regard should not apply to such practices, without prejudice to the power of Digital Services Coordinators to take measures pursuant to this Regulation as regards compliance with this Regulation.
      (68) "It should also include repeatedly requesting a recipient of the service to make a choice where such a choice has already been made, making the procedure of cancelling a service significantly more cumbersome than signing up to it, or making certain choices more difficult or time-consuming than others, making it unreasonably difficult to discontinue purchases or to sign out from a given online platform allowing consumers to conclude distance contracts with traders, and deceiving the recipients of the service by nudging them into decisions on transactions, or by default settings that are very difficult to change, and so unreasonably bias the decision making of the recipient of the service, in a way that distorts and impairs their autonomy, decision-making and choice. However, rules preventing dark patterns should not be understood as preventing providers to interact directly with recipients of the service and to offer new or additional services to them. Legitimate practices, for example in advertising, that are in compliance with Union law should not in themselves be regarded as constituting dark patterns. Those rules on dark patterns should be interpreted as covering prohibited practices falling within the scope of this Regulation to the extent that those practices are not already covered under Directive 2005/29/EC or Regulation (EU) 2016/679."
    
    key_requirements:
      - Do not design, organize, or operate online interfaces in a way that deceives, manipulates, or materially distorts the ability of recipients to make free and informed decisions
      - Avoid specific prohibited practices such as giving more prominence to certain choices when asking for a decision (unequal visual weight)
      - Do not repeatedly request a recipient to make a choice where that choice has already been made (nagging)
      - Ensure the procedure for terminating a service is not more difficult than the procedure for subscribing to it (roach motel)
      - Refrain from using default settings that are very difficult to change or that unreasonably bias decision-making
    
    edge_cases:
      - This Article applies only to online platforms, not to all intermediary services
      - The prohibition does not apply to practices that are already covered by Directive 2005/29/EC (Unfair Commercial Practices Directive) or Regulation (EU) 2016/679 (GDPR); these specific laws take precedence over the general DSA prohibition
      - Legitimate practices, such as persuading users to sign up for new services or direct interaction, are not automatically considered dark patterns provided they do not impair autonomy or deceive

  - article: 26
    title: Advertising on online platforms
    context: |
      (68) In order to increase transparency and to create a safe and predictable environment for recipients of the service, and to prevent unfair commercial practices on online platforms, this Regulation should require providers of online platforms that display advertising on their online interfaces to require those recipients to be able to identify clearly and unambiguously that the information they are viewing is an advertisement. Requirements to identify advertising also apply in relation to advertisements presented as content by recipients of the service themselves, where such recipients have a commercial relationship with the online platform. In particular, influencers on audio or audio-visual media sharing platforms could be subject to such requirements. In order to avoid imposing unnecessary burdens on recipients who are not posting advertisements, the content should be identified as commercial communications only where such recipients have declared it to be so. To that end, online platforms should provide a user-friendly functionality for such declarations to be submitted. In order to facilitate supervision of compliance with this Regulation and to create a high level of transparency on online platforms, information displayed as advertisements, including by so-called influencers, should always clearly and prominently indicate the natural or legal person on whose behalf such advertisement is presented and the natural or legal person who paid for such advertisement, if different from the natural or legal person on whose behalf it is presented. In addition, in order to increase transparency in relation to advertising displayed on online platforms and to enhance the autonomy of recipients of the service, it is appropriate to require online platforms to provide meaningful information regarding the criteria that underpin the advertisement being displayed to that particular recipient, based on profiling or other processing of personal data or without such processing. Such information should be provided by the provider of the online platform in a clear and comprehensive manner, enabling recipients of the service to know what factors they should change or not in order to influence whether or not they will be presented with such an advertisement. In this regard, under Article 21(5) of Regulation (EU) 2016/679, the data subject has the right to object at any time to profiling activities for the purposes of direct marketing. In addition, Article 6(1) of Directive 2002/58/EC provides that consent is required for the storage of information, and gaining of access to information already stored, in the terminal equipment of a user or subscriber. Information about the main parameters can include, for example, that an advertisement is being presented to the recipient of the service on the basis of the fact that the recipient is situated in a particular location, whether inferred or known, the language in which the recipient's online interface is displayed, or the type of device used by the recipient.
      (69) In order to protect minors and to respect their right to protection of their best interests, and considering that minors are generally less capable of assessing the consequences of being profiled and of defending themselves, it is necessary to prohibit the presentation of advertisements on online platforms based on profiling using personal data of the recipient of the service when the provider of online platforms has information that allows it to conclude, with reasonable certainty, that the recipient of the service is a minor. The principle of the best interests of the child should be a primary consideration by providers of online platforms when designing, developing and operating their services.
      (70) The prohibition of presenting targeted advertising on the basis of special categories of personal data, in line with Article 9(1) of Regulation (EU) 2016/679, enhances the protection of recipients of the service as regards the risks that such advertising can create.
    key_requirements:
      - Ensure that for each specific advertisement, recipients can identify in real-time that the information is an advertisement, including through prominent markings
      - Clearly identify the natural or legal person on whose behalf the advertisement is presented (the advertiser)
      - Identify the natural or legal person who paid for the advertisement, if different from the advertiser
      - Provide meaningful information directly and easily accessible from the advertisement about the main parameters used to determine the recipient (targeting criteria) and how to change them
      - Provide a functionality for recipients (e.g., influencers) to declare whether the content they provide is or contains commercial communications
      - Do not present advertisements based on profiling using special categories of personal data (e.g., health, political opinion, sexual orientation) as defined in Article 9(1) of the GDPR

    edge_cases:
      - The prohibition on profiling based on *minors'* data is often discussed here due to Recital 69, but it is legally housed in Article 28(2); Article 26(3) specifically covers 'special categories' of data for *all* users
      - When a user (influencer) declares content as commercial using the mandated functionality, the platform must ensure the standard ad disclosures (ad label, identity of payer/advertiser) appear on that content
      - The transparency regarding 'main parameters' requires explaining the logic of the profiling (e.g., 'you are seeing this because you are in location X and visited site Y'), not just a generic statement

  - article: 27
    title: Recommender system transparency
    context: |
      (70) "A core part of the online platform’s business is the manner in which information is prioritised and presented on its online interface to facilitate and optimise access to information for the recipients of the service. This is done, for example, by algorithmically suggesting, ranking and prioritising information, distinguishing through text or other visual representations, or otherwise curating information provided by recipients. Such recommender systems can have a significant impact on the ability of recipients to retrieve and interact with information online, including to facilitate the search of relevant information for recipients of the service and contribute to an improved user experience. They also play an important role in the amplification of certain messages, the viral dissemination of information and the stimulation of online behaviour. Consequently, online platforms should consistently ensure that recipients of their service are appropriately informed about how recommender systems impact the way information is displayed, and can influence how information is presented to them. They should clearly present the parameters for such recommender systems in an easily comprehensible manner to ensure that the recipients of the service understand how information is prioritised for them. Those parameters should include at least the most important criteria in determining the information suggested to the recipient of the service and the reasons for their respective importance, including where information is prioritised based on profiling and their online behaviour."
      (71) Recommender systems play an important role in the amplification of certain messages and in the viral dissemination of information, thereby having a significant impact on the ability of recipients of the service to retrieve and interact with information online. Such systems can also significantly influence the online debate, and the choices of recipients of the service, for instance what media they consume, and to have an impact on other persons' fundamental rights, personal security, and the functioning of democratic processes and public security. Such systems should, therefore, be transparent and recipients of the service should be provided with appropriate levels of transparency and control over the functioning of these systems. While recommender systems used by online platforms can take various forms, the rules of this Regulation should apply to all of them, in order to ensure a high level of transparency and accountability. This Regulation should not affect recommender systems that are deployed for the sole purpose of facilitating the requested or initiated task by the recipient of the service. In particular, this includes baseline recommender systems including, for example, recommender systems that merely sort or organise information provided by recipients of the service based on parameters explicitly requested or selected by the recipient of the service, such as alphabetical ordering or chronological ordering, and lists based on explicit choices by the recipients such as filters and or search tools. Such baseline recommender systems should thus not fall within the scope of the transparency obligation for recommender systems laid down in this Regulation, as they are limited to organising information in a manner requested by the recipient of the service and do not use prioritisation using means beyond such explicit requests. To ensure the transparency and accountability of automated recommender systems that play a crucial role in mediating access to information and communications, providers of online platforms should clearly present to recipients of their service the main parameters for recommender systems in their terms and conditions, along with any options for modifying or influencing those parameters. That information should be clear and easily comprehensible for recipients and explain the parameters that determine the information presented to recipients of the service. In addition, online platforms should provide recipients of the service with at least one option for each of their recommender systems, enabling them to modify or influence those parameters.
    key_requirements:
      - Set out in terms and conditions, in plain and intelligible language, the main parameters used in recommender systems
      - Explain the most significant criteria determining suggestions and the reasons for their relative importance
      - Describe any options available to recipients to modify or influence these main parameters
      - Where options exist, provide a functionality to select and modify them that is directly and easily accessible from the interface where recommendations appear

    edge_cases:
      - This transparency obligation does not apply to 'baseline' recommender systems that merely sort information based on explicit user requests (e.g., chronological or alphabetical order) without further algorithmic prioritization
      - Unlike the stricter obligation for VLOPs under Article 38, Article 27 does not explicitly mandate a 'non-profiled' option for all platforms, only transparency and access to existing modification options
      - The explanation of parameters must include how 'profiling' and 'online behaviour' influence the recommendations, ensuring users understand why they see specific content

  - article: 28
    title: Online protection of minors
    context: |
      (69) In order to protect minors and to respect their right to protection of their best interests, and considering that minors are generally less capable of assessing the consequences of being profiled and of defending themselves, it is necessary to prohibit the presentation of advertisements on online platforms based on profiling using personal data of the recipient of the service when the provider of online platforms has information that allows it to conclude, with reasonable certainty, that the recipient of the service is a minor. The principle of the best interests of the child should be a primary consideration by providers of online platforms when designing, developing and operating their services.
      (46) "Providers of intermediary services that are primarily directed at minors, for example through the design or marketing of the service, or which are used predominantly by minors, should make particular efforts to render the explanation of their terms and conditions easily understandable to minors."
      (71) "The protection of minors is an important policy objective of the Union. An online platform can be considered to be accessible to minors when its terms and conditions permit minors to use the service, when its service is directed at or predominantly used by minors, or where the provider is otherwise aware that some of the recipients of its service are minors, for example because it already processes personal data of the recipients of its service revealing their age for other purposes. Providers of online platforms used by minors should take appropriate and proportionate measures to protect minors, for example by designing their online interfaces or parts thereof with the highest level of privacy, safety and security for minors by default where appropriate or adopting standards for protection of minors, or participating in codes of conduct for protecting minors. They should consider best practices and available guidance, such as that provided by the communication of the Commission on A Digital Decade for children and youth: the new European strategy for a better internet for kids (BIK+). Providers of online platforms should not present advertisements based on profiling using personal data of the recipient of the service when they are aware with reasonable certainty that the recipient of the service is a minor. In accordance with Regulation (EU) 2016/679, notably the principle of data minimisation as provided for in Article 5(1), point (c), thereof, this prohibition should not lead the provider of the online platform to maintain, acquire or process more personal data than it already has in order to assess if the recipient of the service is a minor. Thus, this obligation should not incentivize providers of online platforms to collect the age of the recipient of the service prior to their use. It should be without prejudice to Union law on protection of personal data."

    key_requirements:
      - Put in place appropriate and proportionate measures to ensure a high level of privacy, safety, and security of minors on the service
      - Do not present advertisements on the interface based on profiling (using personal data) when the provider is aware with reasonable certainty that the recipient is a minor
      - Ensure compliance without processing additional personal data solely to assess whether the recipient is a minor
      - Consider best practices and guidance, such as the 'Better Internet for Kids' strategy, when designing these measures (Recital 71)

    edge_cases:
      - The obligation applies to platforms 'accessible to' minors, not just those 'directed at' them; this includes services where the provider is otherwise aware that some recipients are minors
      - Article 28(3) creates a 'data minimization shield'. providers are not obliged (and should not be incentivized) to collect age data or verify identity for *all* users just to comply with the minor protection rule, preventing invasive age-gating from becoming the default
      - \"Reasonable certainty\" implies that the ban on targeted ads applies if the provider already has data suggesting the user is a minor (e.g., self-declared age, inferred usage patterns), even if they haven't verified it with ID

  # ============================================================================
  # Section 4: Online Marketplace Obligations (Articles 30-32)
  # Apply to: Online Marketplace only
  # NOTE: NO SME exemption for marketplace obligations
  # ============================================================================


  - article: 29
    title: SME Exemption for Marketplace Obligations
    context: |
      Article 2 Commission Recommendation 2003/361/EC - The category of micro, small and medium-sized enterprises (SMEs) is made up of enterprises which employ fewer than 250 persons and which have an annual turnover not exceeding €50 million, and/or an annual balance sheet total not exceeding €43 million. Within the SME category, a small enterprise is defined as an enterprise which employs fewer than 50 persons and whose annual turnover and/or annual balance sheet total does not exceed €10 million. Within the SME category, a micro enterprise is defined as an enterprise which employs fewer than 10 persons and whose annual turnover and/or annual balance sheet total does not exceed €2 million.
      (69) "To avoid disproportionate burdens, the additional obligations imposed under this Regulation on providers of online platforms, including platforms allowing consumers to conclude distance contracts with traders, should not apply to providers that qualify as micro or small enterprises as defined in Recommendation 2003/361/EC. For the same reason, those additional obligations should also not apply to providers of online platforms that previously qualified as micro or small enterprises during a period of 12 months after they lose that status. Such providers should not be excluded from the obligation to provide information on the average monthly active recipients of the service at the request of the Digital Services Coordinator of establishment or the Commission. However, considering that very large online platforms or very large online search engines have a larger reach and a greater impact in influencing how recipients of the service obtain information and communicate online, such providers should not benefit from that exclusion, irrespective of whether they qualify or recently qualified as micro or small enterprises. The consolidation rules laid down in Recommendation 2003/361/EC help ensure that any circumvention of those additional obligations is prevented. Nothing in this Regulation precludes providers of online platforms that are covered by that exclusion from setting up, on a voluntary basis, a system that complies with one or more of those obligations."

    key_requirements:
      - Exempts providers of online platforms allowing consumers to conclude distance contracts (marketplaces) from the obligations laid down in Section 4 (Articles 30, 31, and 32) if they qualify as micro or small enterprises
      - The exemption continues to apply to providers for a period of 12 months after they lose the status of a micro or small enterprise
      - The exemption does NOT apply if the marketplace has been designated as a Very Large Online Platform (VLOP) pursuant to Article 33, regardless of its employee count or turnover

    edge_cases:
      - Contrary to the assumption that all marketplaces must comply, the DSA specifically shields small marketplaces from the heavy administrative burden of gathering trader documentation (Article 30) and designing compliance interfaces (Article 31) to foster innovation
      - The definition of 'micro or small' is strictly tied to Recommendation 2003/361/EC, which includes consolidation rules; a marketplace owned by a large group cannot claim this exemption even if the subsidiary itself is small
      - Nothing precludes exempt marketplaces from voluntarily adopting these measures to increase trust, but it is not a statutory obligation under the DSA

  - article: 30
    title: Traceability of traders
    context: |
      (72) In order to contribute to a safe, trustworthy and transparent online environment for consumers, as well as for other interested parties such as competing traders and holders of intellectual property rights, and to deter traders from selling products or services in violation of the applicable rules, online platforms allowing consumers to conclude distance contracts with traders should ensure that such traders are traceable. The trader should therefore be required to provide certain essential information to the providers of online platforms allowing consumers to conclude distance contracts with traders, including for purposes of promoting messages on or offering products. That requirement should also be applicable to traders that promote messages on products or services on behalf of brands, based on underlying agreements. Those providers of online platforms should store all information in a secure manner for the duration of their contractual relationship with the trader and 6 months thereafter, to allow any claims to be filed against the trader or orders related to the trader to be complied with. This obligation is necessary and proportionate, so that the information can be accessed, in accordance with the applicable law, including on the protection of personal data, by public authorities and private parties with a legitimate interest, including through the orders to provide information referred to in this Regulation. This obligation leaves unaffected potential obligations to preserve certain content for longer periods of time, on the basis of other Union law or national laws, in compliance with Union law. Without prejudice to the definition provided for in this Regulation, any trader, irrespective of whether it is a natural or legal person, identified on the basis of Article 6a(1), point (b), of Directive 2011/83/EU and Article 7(4), point (f), of Directive 2005/29/EC should be traceable when offering a product or service through an online platform. Directive 2000/31/EC obliges all information society services providers to render easily, directly and permanently accessible to the recipients of the service and competent authorities certain information allowing the identification of all providers. The traceability requirements for providers of online platforms allowing consumers to conclude distance contracts with traders set out in this Regulation do not affect the application of Council Directive (EU) 2021/514 (30), which pursues other legitimate public interest objectives.
      (73) To ensure an efficient and adequate application of that obligation, without imposing any disproportionate burdens, providers of online platforms allowing consumers to conclude distance contracts with traders should make best efforts to assess the reliability of the information provided by the traders concerned, in particular by using freely available official online databases and online interfaces, such as national trade registers and the VAT Information Exchange System, or request the traders concerned to provide trustworthy supporting documents, such as copies of identity documents, certified payment accounts statements, company certificates and trade register certificates. They may also use other sources, available for use at a distance, which offer a similar degree of reliability for the purpose of complying with this obligation. However, the providers of online platforms concerned should not be required to engage in excessive or costly online fact-finding exercises or to carry out disproportionate verifications on the spot. Nor should such providers, which have made the best efforts required by this Regulation, be understood as guaranteeing the reliability of the information towards consumer or other interested parties.(74) Given the ease and speed with which illegal products can be offered for sale online, it is important to establish harmonised due diligence obligations for providers of online platforms allowing consumers to conclude distance contracts with traders, so-called 'online marketplaces'. Those obligations should aim to ensure that traders, before they are allowed to offer their products or services through that online platforms, provide basic and reliable information to those providers, in order to make it possible to identify and contact them ex post, as well as identify the specific products or services offered. The basic information should include the name, address, telephone number and email address of the trader; a copy of the identification document of the trader or any other form of electronic identification or European Digital Identity Wallet, as defined by Regulation (EU) No 910/2014 of the European Parliament and of the Council (39); the payment account details of the trader; where the trader is registered in a trade register or similar public register, the trade register in which the trader is registered and its registration number or equivalent means of identification in that register; and a self-certification by the trader committing to only offer products or services that comply with the applicable rules of Union law. Providers of online platforms allowing consumers to conclude distance contracts with traders should make best efforts to verify that the information is reliable, for example by checking official online databases. The online interfaces should be user-friendly and easily accessible for traders and consumers. Additionally and after allowing the offering of the product or service by the trader, the providers of online platforms concerned should make reasonable efforts to randomly check whether the products or services offered have been identified as being illegal in any official, freely accessible and machine-readable online databases or online interfaces available in a Member State or in the Union. The Commission should also encourage traceability of products through technology solutions such as digitally signed Quick Response codes (or 'QR codes') or non-fungible tokens. The Commission should promote the development of standards and, in the absence of them, of market led solutions which can be acceptable to the parties concerned.
    key_requirements:
      - Ensure that traders can only use the platform to offer products or services to consumers if they provide specific identifying information prior to use
      - Collect mandatory data [name, address, telephone, email, copy of identification document (or electronic ID), payment account details, trade register details (if applicable), and self-certification of compliance]
      - Make best efforts to assess the reliability of the information provided using freely available official online databases (e.g., VIES, trade registers) or supporting documents
      - Make the information regarding the trader's identity, trade register number, and self-certification available to recipients of the service in a clear, easily accessible manner
      - Store the collected information securely for the duration of the contractual relationship plus six months
      - Suspend the provision of services to traders who fail to correct inaccurate or incomplete information upon request

    edge_cases:
      - "This obligation applies specifically to 'traders' (professionals); the platform must distinguish between traders and consumer sellers (C2C), as the latter are not subject to these traceability requirements"
      - "'Best efforts' verification does not require excessive or costly online fact-finding exercises or on-site verifications; the platform is not a guarantor of the trader's reliability towards consumers"
      - "For traders already using the platform before 17 February 2024, the provider has a transitional period of 12 months to obtain the required information"

  - article: 31
    title: Compliance by design
    context: |
      (72) "In order to contribute to a safe, trustworthy and transparent online environment for consumers, as well as for other interested parties such as competing traders and holders of intellectual property rights, and to deter traders from selling products or services in violation of the applicable rules, online platforms allowing consumers to conclude distance contracts with traders should ensure that such traders are traceable. The trader should therefore be required to provide certain essential information to the providers of online platforms allowing consumers to conclude distance contracts with traders, including for purposes of promoting messages on or offering products. That requirement should also be applicable to traders that promote messages on products or services on behalf of brands, based on underlying agreements. Those providers of online platforms should store all information in a secure manner for the duration of their contractual relationship with the trader and 6 months thereafter, to allow any claims to be filed against the trader or orders related to the trader to be complied with. This obligation is necessary and proportionate, so that the information can be accessed, in accordance with the applicable law, including on the protection of personal data, by public authorities and private parties with a legitimate interest, including through the orders to provide information referred to in this Regulation. This obligation leaves unaffected potential obligations to preserve certain content for longer periods of time, on the basis of other Union law or national laws, in compliance with Union law. Without prejudice to the definition provided for in this Regulation, any trader, irrespective of whether it is a natural or legal person, identified on the basis of Article 6a(1), point (b), of Directive 2011/83/EU and Article 7(4), point (f), of Directive 2005/29/EC should be traceable when offering a product or service through an online platform. Directive 2000/31/EC obliges all information society services providers to render easily, directly and permanently accessible to the recipients of the service and competent authorities certain information allowing the identification of all providers. The traceability requirements for providers of online platforms allowing consumers to conclude distance contracts with traders set out in this Regulation do not affect the application of Council Directive (EU) 2021/514 (30), which pursues other legitimate public interest objectives."
      (73) "To ensure an efficient and adequate application of that obligation, without imposing any disproportionate burdens, providers of online platforms allowing consumers to conclude distance contracts with traders should make best efforts to assess the reliability of the information provided by the traders concerned, in particular by using freely available official online databases and online interfaces, such as national trade registers and the VAT Information Exchange System, or request the traders concerned to provide trustworthy supporting documents, such as copies of identity documents, certified payment accounts’ statements, company certificates and trade register certificates. They may also use other sources, available for use at a distance, which offer a similar degree of reliability for the purpose of complying with this obligation. However, the providers of online platforms concerned should not be required to engage in excessive or costly online fact-finding exercises or to carry out disproportionate verifications on the spot. Nor should such providers, which have made the best efforts required by this Regulation, be understood as guaranteeing the reliability of the information towards consumer or other interested parties."
      (74) "Providers of online platforms allowing consumers to conclude distance contracts with traders should design and organise their online interface in a way that enables traders to comply with their obligations under relevant Union law, in particular the requirements set out in Articles 6 and 8 of Directive 2011/83/EU, Article 7 of Directive 2005/ 29/EC, Articles 5 and 6 of Directive 2000/31/EC and Article 3 of Directive 98/6/EC of the European Parliament and of the Council (31). For that purpose, the providers of online platforms concerned should make best efforts to assess whether the traders using their services have uploaded complete information on their online interfaces, in line with relevant applicable Union law. The providers of online platforms should ensure that products or services are not offered as long as such information is not complete. This should not amount to an obligation for the providers of online platforms concerned to generally monitor the products or services offered by traders through their services nor a general fact-finding obligation, in particular to assess the accuracy of the information provided by traders. The online interfaces should be user-friendly and easily accessible for traders and consumers. Additionally and after allowing the offering of the product or service by the trader, the providers of online platforms concerned should make reasonable efforts to randomly check whether the products or services offered have been
    key_requirements:
      - Design and organize the online interface to enable traders to comply with their obligations regarding pre-contractual information, compliance, and product safety under Union law
      - Ensure the interface specifically allows traders to provide name, address, contact details, product identification, signs identifying the trader, and applicable labeling/marking info
      - Make best efforts to assess whether the trader has provided the complete information required prior to allowing them to offer products or services
      - Ensure that products or services are not offered on the platform as long as the mandatory information is incomplete
      - Make reasonable efforts to randomly check against official, freely accessible, machine-readable online databases to identify if offered products or services are illegal

    edge_cases:
      - This obligation requires a 'completeness check' rather than a factual verification of every product claim; it explicitly does not constitute a general monitoring or fact-finding obligation
      - The requirement extends to 'random checks' after the product is listed, introducing a proactive element to product safety that goes beyond standard notice-and-action
      - As with all Section 4 obligations, this Article does not apply to platforms qualifying as Micro or Small Enterprises (MSEs) unless they are designated as VLOPs

  - article: 32
    title: Right to information
    context: |
      (74) "Providers of online platforms allowing consumers to conclude distance contracts with traders should design and organise their online interface in a way that enables traders to comply with their obligations under relevant Union law, in particular the requirements set out in Articles 6 and 8 of Directive 2011/83/EU, Article 7 of Directive 2005/ 29/EC, Articles 5 and 6 of Directive 2000/31/EC and Article 3 of Directive 98/6/EC of the European Parliament and of the Council (31). For that purpose, the providers of online platforms concerned should make best efforts to assess whether the traders using their services have uploaded complete information on their online interfaces, in line with relevant applicable Union law. The providers of online platforms should ensure that products or services are not offered as long as such information is not complete. This should not amount to an obligation for the providers of online platforms concerned to generally monitor the products or services offered by traders through their services nor a general fact-finding obligation, in particular to assess the accuracy of the information provided by traders. The online interfaces should be user-friendly and easily accessible for traders and consumers. Additionally and after allowing the offering of the product or service by the trader, the providers of online platforms concerned should make reasonable efforts to randomly check whether the products or services offered have been identified as being illegal in any official, freely accessible and machine-readable online databases or online interfaces available in a Member State or in the Union. The Commission should also encourage traceability of products through technology solutions such as digitally signed Quick Response codes (or ‘QR codes’) or non-fungible tokens. The Commission should promote the development of standards and, in the absence of them, of market led solutions which can be acceptable to the parties concerned".
      (76) "In order to ensure a safe online marketplace environment and to limit the exposure of consumers to illegal or unsafe products and services, providers of online platforms allowing consumers to conclude distance contracts with traders should directly inform consumers who purchased a product or service through their online interface where that content has been removed or disabled on the grounds that it is illegal or pursuant to an order to remove illegal content. That information should include data on the reasons for the removal or disabling of access, information on the identity of the trader and information on possibilities for redress, to allow consumers who may be entitled to compensation to identify the relevant trader and to contact that trader where needed. Where available, other relevant information that was collected during the vetting procedure according to Article 30 of this Regulation should also be provided. Consumers should be informed without undue delay, but in any event within a maximum of 14 days, so that they are able to seek appropriate remedies, such as compensation for damages suffered as a result of the purchase of the illegal or unsafe product or the use of the illegal service. However, that information should only be provided where the provider of the online platform allowing consumers to conclude distance contracts with traders holds the contact details of the consumer. This Regulation is without prejudice to the rules of national laws on the liability of traders for defective products and without prejudice to consumer rights stemming from Union or national consumer law"    key_requirements:
     
    key_requirements:
      - Inform consumers who purchased an illegal product or service if the provider becomes aware of its illegality, irrespective of the means used to become aware
      - Include specific details in the notification [the fact that the product or service is illegal, the identity of the trader, and any relevant means of redress]
      - If the provider does not have the contact details of all affected consumers, make the information publicly available and easily accessible on the online interface
      - This obligation applies to purchases made within the six months preceding the moment the provider became aware of the illegality

    edge_cases:
      - The obligation applies regardless of how the provider learns of the illegality (e.g., via notice, authority order, or voluntary check), but only covers purchases in the preceding 6-month window
      - As part of the Section 4 obligations, this requirement does not apply to platforms qualifying as Micro or Small Enterprises (MSEs) unless they are designated as VLOPs (Article 29)
      - The Recital (76) clarifies that consumers should ideally be informed 'without undue delay, but in any event within a maximum of 14 days', providing a timeframe for compliance interpretation

  # ============================================================================
  # Section 5: VLOP/VLOSE Obligations (Articles 33-43)
  # Apply to: Very Large Online Platforms (VLOPs) and Very Large Online Search Engines (VLOSEs)
  # Threshold: ≥45 million average monthly active recipients in the EU
  # ============================================================================

  - article: 33
    title: Designation of VLOPs and VLOSEs
    context: |
      (77) "Very large online platforms and very large online search engines may cause societal risks, different in scope and impact from those caused by smaller platforms. Providers of such very large online platforms and of very large online search engines should therefore bear the highest standard of due diligence obligations, proportionate to their societal impact. Once the number of active recipients of an online platform or of active recipients of an online search engine, calculated as an average over a period of six months, reaches a significant share of the Union population, the systemic risks the online platform or online search engine poses may have a disproportionate impact in the Union. Such significant reach should be considered to exist where such number exceeds an operational threshold set at 45 million, that is, a number equivalent to 10 % of the Union population. This operational threshold should be kept up to date and therefore the Commission should be empowered to supplement the provisions of this Regulation by adopting delegated acts, where necessary."
      (78) "In view of the network effects characterising the platform economy, the user base of an online platform or an online search engine may quickly expand and reach the dimension of a very large online platform or a very large online search engine, with the related impact on the internal market. This may be the case in the event of exponential growth experienced in short periods of time, or by a large global presence and turnover allowing the online platform or the online search engine to fully exploit network effects and economies of scale and of scope. A high annual turnover or market capitalisation can in particular be an indication of fast scalability in terms of user reach. In those cases, the Digital Services Coordinator of establishment or the Commission should be able to request more frequent reporting from the provider of the online platform or of the online search engine on the number of active recipients of the service to be able to timely identify the moment at which that platform or that search engine should be designated as a very large online platform or very large online search engine, respectively, for the purposes of this Regulation."
    key_requirements:
      - The Commission designates Very Large Online Platforms (VLOPs) and Very Large Online Search Engines (VLOSEs) that reach a threshold of 45 million average monthly active recipients in the Union
      - The designation is made by a Commission decision based on data reported by the provider (under Article 24) or other available information
      - If the Commission bases its designation on 'other information' (e.g., if the provider failed to report), the provider has 10 working days to submit views on the preliminary findings
      - The specific obligations for VLOPs/VLOSEs (Section 5) start to apply four months after the notification of the designation decision to the provider
      - The Commission must terminate the designation if the provider does not meet the 45 million threshold for an uninterrupted period of one year

    edge_cases:
      - Failure to comply with reporting obligations (Article 24) does not prevent designation; the Commission can proceed based on its own estimates or external data to ensure providers cannot escape supervision by hiding data
      - The 45 million threshold is not static; it is effectively 10% of the Union population and can be adjusted by the Commission via delegated acts if the population changes significantly
      - The 'active recipient' metric is specific and defined by delegated acts; it is not necessarily the same as 'registered users' or 'visitors', requiring careful calculation methodology

  - article: 34
    title: Risk assessment
    context: |
      (79) "Very large online platforms and very large online search engines can be used in a way that strongly influences safety online, the shaping of public opinion and discourse, as well as online trade. The way they design their services is generally optimised to benefit their often advertising-driven business models and can cause societal concerns. Effective regulation and enforcement is necessary in order to effectively identify and mitigate the risks and the societal and economic harm that may arise. Under this Regulation, providers of very large online platforms and of very large online search engines should therefore assess the systemic risks stemming from the design, functioning and use of their services, as well as from potential misuses by the recipients of the service, and should take appropriate mitigating measures in observance of fundamental rights. In determining the significance of potential negative effects and impacts, providers should consider the severity of the potential impact and the probability of all such systemic risks. For example, they could assess whether the potential negative impact can affect a large number of persons, its potential irreversibility, or how difficult it is to remedy and restore the situation prevailing prior to the potential impact."
      (80) "Four categories of systemic risks should be assessed in-depth by the providers of very large online platforms and of very large online search engines. A first category concerns the risks associated with the dissemination of illegal content, such as the dissemination of child sexual abuse material or illegal hate speech or other types of misuse of their services for criminal offences, and the conduct of illegal activities, such as the sale of products or services prohibited by Union or national law, including dangerous or counterfeit products, or illegally-traded animals. For example, such dissemination or activities may constitute a significant systemic risk where access to illegal content may spread rapidly and widely through accounts with a particularly wide reach or other means of amplification. Providers of very large online platforms and of very large online search engines should assess the risk of dissemination of illegal content irrespective of whether or not the information is also incompatible with their terms and conditions. This assessment is without prejudice to the personal responsibility of the recipient of the service of very large online platforms or of the owners of websites indexed by very large online search engines for possible illegality of their activity under the applicable law."
      (81) "A second category concerns the actual or foreseeable negative effects of the service on the exercise of fundamental rights, as protected by the Charter, including but not limited to human dignity, freedom of expression and of information, including media freedom and pluralism, the right to private life, data protection, the right to non-discrimination, the rights of the child and consumer protection. Such risks may arise, for example, in relation to the design of the algorithmic systems used by the very large online platform or by the very large online search engine or the misuse of their service through the submission of abusive notices or other methods for silencing speech or hampering competition. When assessing risks to the rights of the child, providers of very large online platforms and of very large online search engines should consider for example how easy it is for minors to understand the design and functioning of the service, as well as how minors can be exposed through their service to content that may impair minors’ health, physical, mental and moral development. Such risks may arise, for example, in relation to the design of online interfaces which intentionally or unintentionally exploit the weaknesses and inexperience of minors or which may cause addictive behaviour."
      (82) "A third category of risks concerns the actual or foreseeable negative effects on democratic processes, civic discourse and electoral processes, as well as public security."
      (83) "A fourth category of risks stems from similar concerns relating to the design, functioning or use, including through manipulation, of very large online platforms and of very large online search engines with an actual or foreseeable negative effect on the protection of public health, minors and serious negative consequences to a person's physical and mental well-being, or on gender-based violence. Such risks may also stem from coordinated disinformation campaigns related to public health, or from online interface design that may stimulate behavioural addictions of recipients of the service."
      (84) "When assessing such systemic risks, providers of very large online platforms and of very large online search engines should focus on the systems or other elements that may contribute to the risks, including all the algorithmic systems that may be relevant, in particular their recommender systems and advertising systems, paying attention to the related data collection and use practices. They should also assess whether their terms and conditions and the enforcement thereof are appropriate, as well as their content moderation processes, technical tools and allocated resources. When assessing the systemic risks identified in this Regulation, those providers should also focus on the information which is not illegal, but contributes to the systemic risks identified in this Regulation. Such providers should therefore pay particular attention on how their services are used to disseminate or amplify misleading or deceptive content, including disinformation. Where the algorithmic amplification of information contributes to the systemic risks, those providers should duly reflect this in their risk assessments. Where risks are localised or there are linguistic differences, those providers should also account for this in their risk assessments. Providers of very large online platforms and of very large online search engines should, in particular, assess how the design and functioning of their service, as well as the intentional and, oftentimes, coordinated manipulation and use of their services, or the systemic infringement of their terms of service, contribute to such risks. Such risks may arise, for example, through the inauthentic use of the service, such as the creation of fake accounts, the use of bots or deceptive use of a service, and other automated or partially automated behaviours, which may lead to the rapid and widespread dissemination to the public of information that is illegal content or incompatible with an online platform’s or online search engine's terms and conditions and that contributes to disinformation campaigns."
      (85) "In order to make it possible that subsequent risk assessments build on each other and show the evolution of the risks identified, as well as to facilitate investigations and enforcement actions, providers of very large online platforms and of very large online search engines should preserve all supporting documents relating to the risk assessments that they carried out, such as information regarding the preparation thereof, underlying data and data on the testing of their algorithmic systems."
    key_requirements:
      - Diligently identify, analyze, and assess systemic risks in the Union stemming from the design, functioning, and use of the service at least once a year
      - Perform risk assessments prior to deploying any functionalities that are likely to have a critical impact on the risks identified
      - Assess specific categories of systemic risks [dissemination of illegal content, negative effects on fundamental rights, negative effects on civic discourse and elections, and risks to public health, minors, and physical/mental well-being]
      - Analyze how specific systems influence these risks, including recommender systems, algorithmic systems, content moderation, advertising systems, and data practices
      - Assess risks related to intentional manipulation of the service, such as inauthentic use or automated exploitation (bots)
      - Preserve all supporting documents and data related to the risk assessments for at least three years

    edge_cases:
    - The assessment scope extends beyond illegal content to include 'lawful but harmful' content (e.g., disinformation) if it contributes to systemic risks like negative effects on civic discourse or public health
    - Assessments must be granular enough to account for specific regional or linguistic differences within the Member States, rather than just a generic EU-wide average
    - This is a dynamic obligation; the 'prior to deploying' clause means major feature updates (like launching a new AI tool or changing the algorithm) trigger a mandatory pre-launch risk assessment

  - article: 35
    title: Mitigation of risks
    context: |
      (86) "Providers of very large online platforms and of very large online search engines should deploy the necessary means to diligently mitigate the systemic risks identified in the risk assessments, in observance of fundamental rights. Any measures adopted should respect the due diligence requirements of this Regulation and be reasonable and effective in mitigating the specific systemic risks identified. They should be proportionate in light of the economic capacity of the provider of the very large online platform or of the very large online search engine and the need to avoid unnecessary restrictions on the use of their service, taking due account of potential negative effects on those fundamental rights. Those providers should give particular consideration to the impact on freedom of expression."
      (87) "Providers of very large online platforms and of very large online search engines should consider under such mitigating measures, for example, adapting any necessary design, feature or functioning of their service, such as the online interface design. They should adapt and apply their terms and conditions, as necessary, and in accordance with the rules of this Regulation on terms and conditions. Other appropriate measures could include adapting their content moderation systems and internal processes or adapting their decision-making processes and resources, including the content moderation personnel, their training and local expertise. This concerns in particular the speed and quality of processing of notices. In this regard, for example, the Code of conduct on countering illegal hate speech online of 2016 sets a benchmark to process valid notifications for removal of illegal hate speech in less than 24 hours. Providers of very large online platforms, in particular those primarily used for the dissemination to the public of pornographic content, should diligently meet all their obligations under this Regulation in respect of illegal content constituting cyber violence, including illegal pornographic content, especially with regard to ensuring that victims can effectively exercise their rights in relation to content representing non-consensual sharing of intimate or manipulated material through the rapid processing of notices and removal of such content without undue delay. Other types of illegal content may require longer or shorter timelines for processing of notices, which will depend on the facts, circumstances and types of illegal content at hand. Those providers may also initiate or increase cooperation with trusted flaggers and organise training sessions and exchanges with trusted flagger organisations."
      (88) "Providers of very large online platforms and of very large online search engines should also be diligent in the measures they take to test and, where necessary, adapt their algorithmic systems, not least their recommender systems. They may need to mitigate the negative effects of personalised recommendations and correct the criteria used in their recommendations. The advertising systems used by providers of very large online platforms and of very large online search engines can also be a catalyser for the systemic risks. Those providers should consider corrective measures, such as discontinuing advertising revenue for specific information, or other actions, such as improving the visibility of authoritative information sources, or more structurally adapting their advertising systems. Providers of very large online platforms and of very large online search engines may need to reinforce their internal processes or supervision of any of their activities, in particular as regards the detection of systemic risks, and conduct more frequent or targeted risk assessments related to new functionalities. In particular, where risks are shared across different online platforms or online search engines, they should cooperate with other service providers, including by initiating or joining existing codes of conduct or other self-regulatory measures. They should also consider awareness-raising actions, in particular where risks relate to disinformation campaigns."
      (89) "Providers of very large online platforms and of very large online search engines should take into account the best interests of minors in taking measures such as adapting the design of their service and their online interface, especially when their services are aimed at minors or predominantly used by them. They should ensure that their services are organised in a way that allows minors to access easily mechanisms provided for in this Regulation, where applicable, including notice and action and complaint mechanisms. They should also take measures to protect minors from content that may impair their physical, mental or moral development and provide tools that enable conditional access to such information. In selecting the appropriate mitigation measures, providers can consider, where appropriate, industry best practices, including as established through self-regulatory cooperation, such as codes of conduct, and should take into account the guidelines from the Commission."
      (90) "Providers of very large online platforms and of very large online search engines should ensure that their approach to risk assessment and mitigation is based on the best available information and scientific insights and that they test their assumptions with the groups most impacted by the risks and the measures they take. To this end, they should, where appropriate, conduct their risk assessments and design their risk mitigation measures with the involvement of representatives of the recipients of the service, representatives of groups potentially impacted by their services, independent experts and civil society organisations. They should seek to embed such consultations into their methodologies for assessing the risks and designing mitigation measures, including, as appropriate, surveys, focus groups, round tables, and other consultation and design methods. In the assessment on whether a measure is reasonable, proportionate and effective, special consideration should be given to the right to freedom of expression."
    key_requirements:
      - Put in place reasonable, proportionate, and effective mitigation measures tailored to the specific systemic risks identified in the risk assessment
      - Adapt the design, features, or functioning of the service, including the online interface and algorithmic systems (recommenders)
      - Adapt content moderation processes, including the speed and quality of processing notices, decision-making processes, and dedicated resources
      - Adapt advertising systems and adopt targeted measures to limit or adjust the presentation of advertisements
      - Take specific measures to protect the rights of the child, including age verification and parental control tools where appropriate
      - Ensure generated or manipulated images/audio/video (deepfakes) are distinguishable through prominent markings

    edge_cases:
      - Mitigation measures must be proportionate to the provider's economic capacity and must strictly observe fundamental rights, particularly freedom of expression; over-blocking is a compliance risk
      - Recital 87 highlights a specific urgency for 'cyber violence' and non-consensual sharing of intimate material (NCII), suggesting that 'diligence' for these categories requires faster action than standard content
      - Providers should not design mitigation measures in a vacuum; they are expected to involve representatives of affected groups, independent experts, and civil society in the design process
  
  - article: 36
    title: Crisis response mechanism
    context: |
     (91) "In times of crisis, there might be a need for certain specific measures to be taken urgently by providers of very large online platforms, in addition to measures they would be taking in view of their other obligations under this Regulation. In that regard, a crisis should be considered to occur when extraordinary circumstances occur that can lead to a serious threat to public security or public health in the Union or significant parts thereof. Such crises could result from armed conflicts or acts of terrorism, including emerging conflicts or acts of terrorism, natural disasters such as earthquakes and hurricanes, as well as from pandemics and other serious cross-border threats to public health. The Commission should be able to require, upon recommendation by the European Board for Digital Services (‘the Board’), providers of very large online platforms and providers of very large search engines to initiate a crisis response as a matter of urgency. Measures that those providers may identify and consider applying may include, for example, adapting content moderation processes and increasing the resources dedicated to content moderation, adapting terms and conditions, relevant algorithmic systems and advertising systems, further intensifying cooperation with trusted flaggers, taking awareness-raising measures and promoting trusted information and adapting the design of their online interfaces. The necessary requirements should be provided for to ensure that such measures are taken within a very short time frame and that the crisis response mechanism is only used where, and to the extent that, this is strictly necessary and any measures taken under this mechanism are effective and proportionate, taking due account of the rights and legitimate interests of all parties concerned. The use of the mechanism should be without prejudice to the other provisions of this Regulation, such as those on risk assessments and mitigation measures and the enforcement thereof and those on crisis protocols."
      (79) "Very large online platforms and very large online search engines can be used in a way that strongly influences safety online, the shaping of public opinion and discourse, as well as online trade. The way they design their services is generally optimised to benefit their often advertising-driven business models and can cause societal concerns. Effective regulation and enforcement is necessary in order to effectively identify and mitigate the risks and the societal and economic harm that may arise. Under this Regulation, providers of very large online platforms and of very large online search engines should therefore assess the systemic risks stemming from the design, functioning and use of their services, as well as from potential misuses by the recipients of the service, and should take appropriate mitigating measures in observance of fundamental rights. In determining the significance of potential negative effects and impacts, providers should consider the severity of the potential impact and the probability of all such systemic risks. For example, they could assess whether the potential negative impact can affect a large number of persons, its potential irreversibility, or how difficult it is to remedy and restore the situation prevailing prior to the potential impact." []
      (80) "Four categories of systemic risks should be assessed in-depth by the providers of very large online platforms and of very large online search engines. A first category concerns the risks associated with the dissemination of illegal content, such as the dissemination of child sexual abuse material or illegal hate speech or other types of misuse of their services for criminal offences, and the conduct of illegal activities, such as the sale of products or services prohibited by Union or national law, including dangerous or counterfeit products, or illegally-traded animals. For example, such dissemination or activities may constitute a significant systemic risk where access to illegal content may spread rapidly and widely through accounts with a particularly wide reach or other means of amplification. Providers of very large online platforms and of very large online search engines should assess the risk of dissemination of illegal content irrespective of whether or not the information is also incompatible with their terms and conditions. This assessment is without prejudice to the personal responsibility of the recipient of the service of very large online platforms or of the owners of websites indexed by very large online search engines for possible illegality of their activity under the applicable law." []
      (81) "A second category concerns the actual or foreseeable negative effects of the service on the exercise of fundamental rights, as protected by the Charter, including but not limited to human dignity, freedom of expression and of information, including media freedom and pluralism, the right to private life, data protection, the right to non-discrimination, the rights of the child and consumer protection. Such risks may arise, for example, in relation to the design of the algorithmic systems used by the very large online platform or by the very large online search engine or the misuse of their service through the submission of abusive notices or other methods for silencing speech or hampering competition. When assessing risks to the rights of the child, providers of very large online platforms and of very large online search engines should consider for example how easy it is for minors to understand the design and functioning of the service, as well as how minors can be exposed through their service to content that may impair minors’ health, physical, mental and moral development. Such risks may arise, for example, in relation to the design of online interfaces which intentionally or unintentionally exploit the weaknesses and inexperience of minors or which may cause addictive behaviour." [,]
      (82) "A third category of risks concerns the actual or foreseeable negative effects on democratic processes, civic discourse and electoral processes, as well as public security." []
      (83) "A fourth category of risks stems from similar concerns relating to the design, functioning or use, including through manipulation, of very large online platforms and of very large online search engines with an actual or foreseeable negative effect on the protection of public health, minors and serious negative consequences to a person's physical and mental well-being, or on gender-based violence. Such risks may also stem from coordinated disinformation campaigns related to public health, or from online interface design that may stimulate behavioural addictions of recipients of the service." []
      (84) "When assessing such systemic risks, providers of very large online platforms and of very large online search engines should focus on the systems or other elements that may contribute to the risks, including all the algorithmic systems that may be relevant, in particular their recommender systems and advertising systems, paying attention to the related data collection and use practices. They should also assess whether their terms and conditions and the enforcement thereof are appropriate, as well as their content moderation processes, technical tools and allocated resources. When assessing the systemic risks identified in this Regulation, those providers should also focus on the information which is not illegal, but contributes to the systemic risks identified in this Regulation. Such providers should therefore pay particular attention on how their services are used to disseminate or amplify misleading or deceptive content, including disinformation. Where the algorithmic amplification of information contributes to the systemic risks, those providers should duly reflect this in their risk assessments. Where risks are localised or there are linguistic differences, those providers should also account for this in their risk assessments. Providers of very large online platforms and of very large online search engines should, in particular, assess how the design and functioning of their service, as well as the intentional and, oftentimes, coordinated manipulation and use of their services, or the systemic infringement of their terms of service, contribute to such risks. Such risks may arise, for example, through the inauthentic use of the service, such as the creation of fake accounts, the use of bots or deceptive use of a service, and other automated or partially automated behaviours, which may lead to the rapid and widespread dissemination to the public of information that is illegal content or incompatible with an online platform’s or online search engine's terms and conditions and that contributes to disinformation campaigns."
      (85) "In order to make it possible that subsequent risk assessments build on each other and show the evolution of the risks identified, as well as to facilitate investigations and enforcement actions, providers of very large online platforms and of very large online search engines should preserve all supporting documents relating to the risk assessments that they carried out, such as information regarding the preparation thereof, underlying data and data on the testing of their algorithmic systems."
      (108) "In addition to the crisis response mechanism for very large online platforms and very large online search engines, the Commission may initiate the drawing up of voluntary crisis protocols to coordinate a rapid, collective and cross- border response in the online environment. Such can be the case, for example, where online platforms are misused for the rapid spread of illegal content or disinformation or where the need arises for rapid dissemination of reliable information. In light of the important role of very large online platforms in disseminating information in our societies and across borders, providers of such platforms should be encouraged in drawing up and applying specific crisis protocols. Such crisis protocols should be activated only for a limited period of time and the measures adopted should also be limited to what is strictly necessary to address the extraordinary circumstance. Those measures should be consistent with this Regulation, and should not amount to a general obligation for the participating providers of very large online platforms and of very large online search engines to monitor the information which they transmit or store, nor actively to seek facts or circumstances indicating illegal content."

    key_requirements:
      - Compliance with Commission Decision. Upon the adoption of a decision by the Commission, identifying a crisis, the provider must strictly comply with the order to initiate a crisis response
      - Immediate Assessment. Assess whether and how the functioning and use of the service significantly contribute to, or are likely to significantly contribute to, the serious threat identified
      - Emergency Measures. Identify and apply specific, effective, and proportionate measures to prevent, eliminate, or limit any such contribution (e.g., adapting content moderation, algorithmic systems, or advertising systems)
      - Reporting. Report to the Commission by the deadline specified in the decision on the specific measures taken and their outcomes

    edge_cases:
      - Definition of Crisis. This mechanism is triggered only by 'extraordinary circumstances' leading to a serious threat to public security or public health in the Union (e.g., armed conflicts, acts of terrorism, pandemics, or natural disasters)
      - Time Limitation. The Commission's decision to require action is time-bound (maximum 3 months), though it can be renewed if the crisis persists; it is not a permanent state of exception
      - Voluntary vs. Mandatory. While Article 36 is a mandatory instruction from the Commission, Article 48 (Crisis Protocols) covers voluntary coordination; the two often work in tandem, but Article 36 is the enforcement backstop

  - article: 37
    title: Independent audit
    context: |
      (92) "Given the need to ensure verification by independent experts, providers of very large online platforms and of very large online search engines should be accountable, through independent auditing, for their compliance with the obligations laid down by this Regulation and, where relevant, any complementary commitments undertaken pursuant to codes of conduct and crises protocols. In order to ensure that audits are carried out in an effective, efficient and timely manner, providers of very large online platforms and of very large online search engines should provide the necessary cooperation and assistance to the organisations carrying out the audits, including by giving the auditor access to all relevant data and premises necessary to perform the audit properly, including, where appropriate, to data related to algorithmic systems, and by answering oral or written questions. Auditors should also be able to make use of other sources of objective information, including studies by vetted researchers. Providers of very large online platforms and of very large online search engines should not undermine the performance of the audit. Audits should be performed according to best industry practices and high professional ethics and objectivity, with due regard, as appropriate, to auditing standards and codes of practice. Auditors should have the necessary expertise in the area of risk management and technical competence to audit algorithms. They should be independent, in order to be able to perform their tasks in an adequate and trustworthy manner. They should comply with core independence requirements for prohibited non-auditing services, firm rotation and non-contingent fees. If their independence and technical competence is not beyond doubt, they should resign or abstain from the audit engagement."
      (93) "The audit report should be substantiated, in order to give a meaningful account of the activities undertaken and the conclusions reached. It should help inform, and where appropriate suggest improvements to the measures taken by the providers of the very large online platform and of the very large online search engine to comply with their obligations under this Regulation. The audit report should be transmitted to the Digital Services Coordinator of establishment, the Commission and the Board following the receipt of the audit report. Providers should also transmit upon completion without undue delay each of the reports on the risk assessment and the mitigation measures, as well as the audit implementation report of the provider of the very large online platform or of the very large online search engine showing how they have addressed the audit’s recommendations. The audit report should include an audit opinion based on the conclusions drawn from the audit evidence obtained. A ‘positive opinion’ should be given where all evidence shows that the provider of the very large online platform or of the very large online search engine complies with the obligations laid down by this Regulation or, where applicable, any commitments it has undertaken pursuant to a code of conduct or crisis protocol, in particular by identifying, evaluating and mitigating the systemic risks posed by its system and services. A ‘positive opinion’ should be accompanied by comments where the auditor wishes to include remarks that do not have a substantial effect on the outcome of the audit. A ‘negative opinion’ should be given where the auditor considers that the provider of the very large online platform or of the very large online search engine does not comply with this Regulation or the commitments undertaken. Where the audit opinion could not reach a conclusion for specific elements that fall within the scope of the audit, an explanation of reasons for the failure to reach such a conclusion should be included in the audit opinion. Where applicable, the report should include a description of specific elements that could not be audited, and an explanation of why these could not be audited."
    key_requirements:
      - Subject the service to an independent audit at least once a year to assess compliance with the following obligations:
        - Chapter III obligations (excluding Section 5).
        - Obligations under commitments to Codes of Conduct and Crisis Protocols.
        - Risk management obligations (Articles 34 and 35).
      - Ensure the auditing organization is independent (no conflicts of interest, no contingent fees) and possesses proven technical expertise in risk management and algorithm auditing
      - Afford the auditor full access to all relevant data, premises, and algorithmic systems necessary to perform the audit properly
      - Adopt an audit implementation report detailing how the provider addresses the audit's recommendations
      - If the provider does not implement the operational recommendations, they must provide a detailed justification and set out alternative measures

    edge_cases:
      - "'Independence' is strict: the auditor must not have provided non-auditing services to the provider in the 12 months prior or have been involved in the matters being audited"
      - "The audit opinion can be 'positive', 'positive with comments', or 'negative'. A 'negative' opinion requires the provider to take specific actions to remedy non-compliance"
      - "If the auditor cannot audit specific elements (e.g., due to lack of access or technical limitations), they must explain the reasons in the report; this failure can itself signal non-compliance"

  
  - article: 38
    title: Recommender systems (additional transparency)
    context: |
      (94) "The obligations on assessment and mitigation of risks should trigger, on a case-by-case basis, the need for providers of very large online platforms and of very large online search engines to assess and, where necessary, adjust the design of their recommender systems, for example by taking measures to prevent or minimise biases that lead to the discrimination of persons in vulnerable situations, in particular where such adjustment is in accordance with data protection law and when the information is personalised on the basis of special categories of personal data referred to in Article 9 of the Regulation (EU) 2016/679. In addition, and complementing the transparency obligations applicable to online platforms as regards their recommender systems, providers of very large online platforms and of very large online search engines should consistently ensure that recipients of their service enjoy alternative options which are not based on profiling, within the meaning of Regulation (EU) 2016/679, for the main parameters of their recommender systems. Such choices should be directly accessible from the online interface where the recommendations are presented."
      (70) "A core part of the online platform’s business is the manner in which information is prioritised and presented on its online interface to facilitate and optimise access to information for the recipients of the service. This is done, for example, by algorithmically suggesting, ranking and prioritising information, distinguishing through text or other visual representations, or otherwise curating information provided by recipients. Such recommender systems can have a significant impact on the ability of recipients to retrieve and interact with information online, including to facilitate the search of relevant information for recipients of the service and contribute to an improved user experience. They also play an important role in the amplification of certain messages, the viral dissemination of information and the stimulation of online behaviour. Consequently, online platforms should consistently ensure that recipients of their service are appropriately informed about how recommender systems impact the way information is displayed, and can influence how information is presented to them. They should clearly present the parameters for such recommender systems in an easily comprehensible manner to ensure that the recipients of the service understand how information is prioritised for them. Those parameters should include at least the most important criteria in determining the information suggested to the recipient of the service and the reasons for their respective importance, including where information is prioritised based on profiling and their online behaviour."

    key_requirements:
      - In addition to the transparency obligations of Article 27, provide at least one option for each recommender system that is not based on profiling (within the meaning of Article 4(4) of the GDPR)
      - Ensure that the specific parameters for recommender systems (discussed in Article 27) are available and easily modifiable
      - Make the functionality to select or modify these options directly and easily accessible from the specific part of the online interface where the recommendations are presented

    edge_cases:
      - "This obligation is unique to VLOPs and VLOSEs; while all platforms must explain their parameters (Article 27), only VLOPs/VLOSEs *must* offer a non-profiled (e.g., purely chronological or popularity-based) feed"
      - \"'Directly accessible' implies users shouldn't have to dig through deep settings menus to turn off profiling; a toggle or button near the feed is expected\"
      - \"Profiling includes any automated processing of personal data to evaluate personal aspects (preferences, interests, behavior, location); a non-profiled option effectively disables the 'algorithm' personalization loop\"

  - article: 39
    title: Additional online advertising transparency
    context: |
     (68) "Online advertising plays an important role in the online environment, including in relation to the provision of online platforms, where the provision of the service is sometimes in whole or in part remunerated directly or indirectly, through advertising revenues. Online advertising can contribute to significant risks, ranging from advertisements that are themselves illegal content, to contributing to financial incentives for the publication or amplification of illegal or otherwise harmful content and activities online, or the discriminatory presentation of advertisements with an impact on the equal treatment and opportunities of citizens. In addition to the requirements resulting from Article 6 of Directive 2000/31/EC, providers of online platforms should therefore be required to ensure that the recipients of the service have certain individualised information necessary for them to understand when and on whose behalf the advertisement is presented. They should ensure that the information is salient, including through standardised visual or audio marks, clearly identifiable and unambiguous for the average recipient of the service, and should be adapted to the nature of the individual service’s online interface. In addition, recipients of the service should have information directly accessible from the online interface where the advertisement is presented, on the main parameters used for determining that a specific advertisement is presented to them, providing meaningful explanations of the logic used to that end, including when this is based on profiling. Such explanations should include information on the method used for presenting the advertisement, for example whether it is contextual or other type of advertising, and, where applicable, the main profiling criteria used; it should also inform the recipient about any means available for them to change such criteria. The requirements of this Regulation on the provision of information relating to advertising is without prejudice to the application of the relevant provisions of Regulation (EU) 2016/679, in particular those regarding the right to object, automated individual decision-making, including profiling, and specifically the need to obtain consent of the data subject prior to the processing of personal data for targeted advertising. Similarly, it is without prejudice to the provisions laid down in Directive 2002/58/EC in particular those regarding the storage of information in terminal equipment and the access to information stored therein. Finally, this Regulation complements the application of the Directive 2010/13/EU which imposes measures to enable users to declare audiovisual commercial communications in user-generated videos. It also complements the obligations for traders regarding the disclosure of commercial communications deriving from Directive 2005/29/EC."
      (94) "The obligations on assessment and mitigation of risks should trigger, on a case-by-case basis, the need for providers of very large online platforms and of very large online search engines to assess and, where necessary, adjust the design of their recommender systems, for example by taking measures to prevent or minimise biases that lead to the discrimination of persons in vulnerable situations, in particular where such adjustment is in accordance with data protection law and when the information is personalised on the basis of special categories of personal data referred to in Article 9 of the Regulation (EU) 2016/679. In addition, and complementing the transparency obligations applicable to online platforms as regards their recommender systems, providers of very large online platforms and of very large online search engines should consistently ensure that recipients of their service enjoy alternative options which are not based on profiling, within the meaning of Regulation (EU) 2016/679, for the main parameters of their recommender systems. Such choices should be directly accessible from the online interface where the recommendations are presented."
    key_requirements:
      - Compile and make publicly available through a searchable and reliable tool (repository) a register containing specific information about advertisements presented on the interface
      - Ensure the repository contains the content of the advertisement, the natural or legal person on whose behalf it is presented (advertiser), and who paid for it
  - Include the period during which the advertisement was presented and whether it was intended for a specific group of recipients
  - Disclose the main parameters used for targeting recipients (e.g., demographics, interests, location) and, where applicable, the excluded categories
  - Provide the total number of recipients reached and, where applicable, the aggregate numbers broken down by Member State
  - Maintain this information in the repository for one year after the advertisement was presented for the last time

edge_cases:
  - The repository must include *all* ads, including commercial communications declared by influencers (under Article 26), not just paid programmatic display ads
  - The tool must allow multicriteria searching and use application programming interfaces (APIs) to enable researchers and civil society to analyze ad campaigns programmatically
  - The obligation applies specifically to VLOPs and VLOSEs; standard platforms only need to provide transparency to the *individual user* seeing the ad (Article 26), whereas VLOPs must provide a public *archive*

  - article: 40
    title: Data access for researchers
    context: |
      (96) "In order to appropriately monitor and assess the compliance of very large online platforms and of very large online search engines with the obligations laid down by this Regulation, the Digital Services Coordinator of establishment or the Commission may require access to or reporting of specific data, including data related to algorithms. Such a requirement may include, for example, the data necessary to assess the risks and possible harms brought about by the very large online platform’s or the very large online search engine’s systems, data on the accuracy, functioning and testing of algorithmic systems for content moderation, recommender systems or advertising systems, including, where appropriate, training data and algorithms, or data on processes and outputs of content moderation or of internal complaint-handling systems within the meaning of this Regulation. Such data access requests should not include requests to produce specific information about individual recipients of the service for the purpose of determining compliance of such recipients with other applicable Union or national law. Investigations by researchers on the evolution and severity of online systemic risks are particularly important for bridging information asymmetries and establishing a resilient system of risk mitigation, informing providers of online platforms, providers of online search engines, Digital Services Coordinators, other competent authorities, the Commission and the public."
      (97) "This Regulation therefore provides a framework for compelling access to data from very large online platforms and very large online search engines to vetted researchers affiliated to a research organisation within the meaning of Article 2 of Directive (EU) 2019/790, which may include, for the purpose of this Regulation, civil society organisations that are conducting scientific research with the primary goal of supporting their public interest mission. All requests for access to data under that framework should be proportionate and appropriately protect the rights and legitimate interests, including the protection of personal data, trade secrets and other confidential information, of the very large online platform or of the very large online search engine and any other parties concerned, including the recipients of the service. However, to ensure that the objective of this Regulation is achieved, consideration of the commercial interests of providers should not lead to a refusal to provide access to data necessary for the specific research objective pursuant to a request under this Regulation. In this regard, whilst without prejudice to Directive (EU) 2016/943 of the European Parliament and of the Council (32), providers should ensure appropriate access for researchers, including, where necessary, by taking technical protections such as through data vaults."
      (98) "In addition, where data is publicly accessible, such providers should not prevent researchers meeting an appropriate subset of criteria from using this data for research purposes that contribute to the detection, identification and understanding of systemic risks. They should provide access to such researchers including, where technically possible, in real-time, to the publicly accessible data, for example on aggregated interactions with content from public pages, public groups, or public figures, including impression and engagement data such as the number of reactions, shares, comments from recipients of the service. Providers of very large online platforms or of very large online search engines should be encouraged to cooperate with researchers and provide broader access to data for monitoring societal concerns through voluntary efforts, including through commitments and procedures agreed under codes of conduct or crisis protocols. Those providers and researchers should pay particular attention to the protection of personal data, and ensure that any processing of personal data complies with Regulation (EU) 2016/ 679. Providers should anonymise or pseudonymise personal data except in those cases that would render impossible the research purpose pursued."
    key_requirements:
  - Provide the Digital Services Coordinator (DSC) of establishment or the Commission with access to data necessary to monitor and assess compliance with the Regulation upon their reasoned request
  - Provide access to data to 'vetted researchers' for the purpose of conducting research that contributes to the detection, identification, and understanding of systemic risks in the Union
  - Explain the design, logic, the functioning, and the testing of algorithmic systems to the DSC or Commission upon request
  - Provide access to publicly accessible data (e.g., public posts, engagement metrics) to researchers who meet specific criteria, without delay and, where technically possible, in real-time
  - Establish a dedicated point of contact for researchers and authorities to handle these requests

edge_cases:
  - "Access can only be refused on limited grounds: if the request is technically impossible, if the provider cannot comply with national/Union law (e.g., GDPR), or if it reveals trade secrets (though trade secrets alone are not a blanket shield; technical protections like 'data vaults' must be considered first)"
  - "'Vetted researchers' must be affiliated with a research organization, independent from commercial interests, and capable of fulfilling specific data security requirements; not every academic qualifies automatically"
  - "If a provider refuses a request for access, they must provide a reasoned explanation to the DSC; the DSC can then issue an order compelling access"

  - article: 41
    title: Compliance function
    context: |
      (99) "Given the complexity of the functioning of the systems deployed and the systemic risks they present to society, providers of very large online platforms and of very large online search engines should establish a compliance function, which should be independent from the operational functions of those providers. The head of the compliance function should report directly to the management of those providers, including for concerns of non- compliance with this Regulation. The compliance officers that are part of the compliance function should have the necessary qualifications, knowledge, experience and ability to operationalise measures and monitor the compliance with this Regulation within the organisation of the providers of very large online platform or of very large online search engine. Providers of very large online platforms and of very large online search engines should ensure that the compliance function is involved, properly and in a timely manner, in all issues which relate to this Regulation including in the risk assessment and mitigation strategy and specific measures, as well as assessing compliance, where applicable, with commitments made by those providers under the codes of conduct and crisis protocols they subscribe to."
    key_requirements:
  - Establish a compliance function that is independent from the operational functions and composed of one or more compliance officers
  - Ensure compliance officers have the necessary authority, stature, resources, and access to the management body to monitor compliance effectively
  - The head of the compliance function must report directly to the management body of the provider
  - Compliance officers must organize and supervise the provider's activities relating to the independent audit (Article 37) and cooperation with authorities
  - Management must define the governance arrangements of the compliance function to ensure its independence

edge_cases:
  - Protection from Dismissal. Compliance officers shall not be dismissed or penalized for the performance of their tasks, ensuring they can raise red flags without fear of retaliation
  - Remuneration of compliance officers must not be directly linked to the financial performance of the provider, preventing conflicts of interest
  - The management body remains fully responsible for non-compliance; the appointment of an officer does not shift liability away from the corporate leadership

  - article: 42
    title: Transparency reporting (VLOP-enhanced)
    context: |
      (100) "In view of the additional risks relating to their activities and their additional obligations under this Regulation, additional transparency requirements should apply specifically to very large online platforms and very large online search engines, notably to report comprehensively on the risk assessments performed and subsequent measures adopted as provided by this Regulation."
      (77) "Very large online platforms and very large online search engines may cause societal risks, different in scope and impact from those caused by smaller platforms. Providers of such very large online platforms and of very large online search engines should therefore bear the highest standard of due diligence obligations, proportionate to their societal impact. Once the number of active recipients of an online platform or of active recipients of an online search engine, calculated as an average over a period of six months, reaches a significant share of the Union population, the systemic risks the online platform or online search engine poses may have a disproportionate impact in the Union. Such significant reach should be considered to exist where such number exceeds an operational threshold set at 45 million, that is, a number equivalent to 10 % of the Union population. This operational threshold should be kept up to date and therefore the Commission should be empowered to supplement the provisions of this Regulation by adopting delegated acts, where necessary."
      (79) "Very large online platforms and very large online search engines can be used in a way that strongly influences safety online, the shaping of public opinion and discourse, as well as online trade. The way they design their services is generally optimised to benefit their often advertising-driven business models and can cause societal concerns. Effective regulation and enforcement is necessary in order to effectively identify and mitigate the risks and the societal and economic harm that may arise. Under this Regulation, providers of very large online platforms and of very large online search engines should therefore assess the systemic risks stemming from the design, functioning and use of their services, as well as from potential misuses by the recipients of the service, and should take appropriate mitigating measures in observance of fundamental rights. In determining the significance of potential negative effects and impacts, providers should consider the severity of the potential impact and the probability of all such systemic risks. For example, they could assess whether the potential negative impact can affect a large number of persons, its potential irreversibility, or how difficult it is to remedy and restore the situation prevailing prior to the potential impact."
      (80) "Four categories of systemic risks should be assessed in-depth by the providers of very large online platforms and of very large online search engines. A first category concerns the risks associated with the dissemination of illegal content, such as the dissemination of child sexual abuse material or illegal hate speech or other types of misuse of their services for criminal offences, and the conduct of illegal activities, such as the sale of products or services prohibited by Union or national law, including dangerous or counterfeit products, or illegally-traded animals. For example, such dissemination or activities may constitute a significant systemic risk where access to illegal content may spread rapidly and widely through accounts with a particularly wide reach or other means of amplification. Providers of very large online platforms and of very large online search engines should assess the risk of dissemination of illegal content irrespective of whether or not the information is also incompatible with their terms and conditions. This assessment is without prejudice to the personal responsibility of the recipient of the service of very large online platforms or of the owners of websites indexed by very large online search engines for possible illegality of their activity under the applicable law."
      (81) "A second category concerns the actual or foreseeable negative effects of the service on the exercise of fundamental rights, as protected by the Charter, including but not limited to human dignity, freedom of expression and of information, including media freedom and pluralism, the right to private life, data protection, the right to non-discrimination, the rights of the child and consumer protection. Such risks may arise, for example, in relation to the design of the algorithmic systems used by the very large online platform or by the very large online search engine or the misuse of their service through the submission of abusive notices or other methods for silencing speech or hampering competition. When assessing risks to the rights of the child, providers of very large online platforms and of very large online search engines should consider for example how easy it is for minors to understand the design and functioning of the service, as well as how minors can be exposed through their service to content that may impair minors’ health, physical, mental and moral development. Such risks may arise, for example, in relation to the design of online interfaces which intentionally or unintentionally exploit the weaknesses and inexperience of minors or which may cause addictive behaviour."
      (82) "A third category of risks concerns the actual or foreseeable negative effects on democratic processes, civic discourse and electoral processes, as well as public security."
      (83) "A fourth category of risks stems from similar concerns relating to the design, functioning or use, including through manipulation, of very large online platforms and of very large online search engines with an actual or foreseeable negative effect on the protection of public health, minors and serious negative consequences to a person's physical and mental well-being, or on gender-based violence. Such risks may also stem from coordinated disinformation campaigns related to public health, or from online interface design that may stimulate behavioural addictions of recipients of the service."
      (84) "When assessing such systemic risks, providers of very large online platforms and of very large online search engines should focus on the systems or other elements that may contribute to the risks, including all the algorithmic systems that may be relevant, in particular their recommender systems and advertising systems, paying attention to the related data collection and use practices. They should also assess whether their terms and conditions and the enforcement thereof are appropriate, as well as their content moderation processes, technical tools and allocated resources. When assessing the systemic risks identified in this Regulation, those providers should also focus on the information which is not illegal, but contributes to the systemic risks identified in this Regulation. Such providers should therefore pay particular attention on how their services are used to disseminate or amplify misleading or deceptive content, including disinformation. Where the algorithmic amplification of information contributes to the systemic risks, those providers should duly reflect this in their risk assessments. Where risks are localised or there are linguistic differences, those providers should also account for this in their risk assessments. Providers of very large online platforms and of very large online search engines should, in particular, assess how the design and functioning of their service, as well as the intentional and, oftentimes, coordinated manipulation and use of their services, or the systemic infringement of their terms of service, contribute to such risks. Such risks may arise, for example, through the inauthentic use of the service, such as the creation of fake accounts, the use of bots or deceptive use of a service, and other automated or partially automated behaviours, which may lead to the rapid and widespread dissemination to the public of information that is illegal content or incompatible with an online platform’s or online search engine's terms and conditions and that contributes to disinformation campaigns."
      (85) "In order to make it possible that subsequent risk assessments build on each other and show the evolution of the risks identified, as well as to facilitate investigations and enforcement actions, providers of very large online platforms and of very large online search engines should preserve all supporting documents relating to the risk assessments that they carried out, such as information regarding the preparation thereof, underlying data and data on the testing of their algorithmic systems."
      (93) "The audit report should be substantiated, in order to give a meaningful account of the activities undertaken and the conclusions reached. It should help inform, and where appropriate suggest improvements to the measures taken by the providers of the very large online platform and of the very large online search engine to comply with their obligations under this Regulation. The audit report should be transmitted to the Digital Services Coordinator of establishment, the Commission and the Board following the receipt of the audit report. Providers should also transmit upon completion without undue delay each of the reports on the risk assessment and the mitigation measures, as well as the audit implementation report of the provider of the very large online platform or of the very large online search engine showing how they have addressed the audit’s recommendations. The audit report should include an audit opinion based on the conclusions drawn from the audit evidence obtained. A ‘positive opinion’ should be given where all evidence shows that the provider of the very large online platform or of the very large online search engine complies with the obligations laid down by this Regulation or, where applicable, any commitments it has undertaken pursuant to a code of conduct or crisis protocol, in particular by identifying, evaluating and mitigating the systemic risks posed by its system and services. A ‘positive opinion’ should be accompanied by comments where the auditor wishes to include remarks that do not have a substantial effect on the outcome of the audit. A ‘negative opinion’ should be given where the auditor considers that the provider of the very large online platform or of the very large online search engine does not comply with this Regulation or the commitments undertaken. Where the audit opinion could not reach a conclusion for specific elements that fall within the scope of the audit, an explanation of reasons for the failure to reach such a conclusion should be included in the audit opinion. Where applicable, the report should include a description of specific elements that could not be audited, and an explanation of why these could not be audited."

    key_requirements:
      - Publish transparency reports every six months containing all information required by Article 15 (Section 2) and Article 24 (Section 3), plus specific VLOP metrics
      - Include detailed human resources data dedicated to content moderation, broken down by each official language of the Member States and providing qualifications and training details
      - Publish the annual risk assessment reports (Article 34) and mitigation measure reports (Article 35)
      - Publish the audit report and the audit implementation report (Article 37) annually
      - Transmit the complete, unredacted reports to the Digital Services Coordinator of establishment and the Commission upon publication

    edge_cases:
      - Confidentiality Redaction. Providers may redact information from public reports if it contains confidential data, trade secrets, or creates security vulnerabilities, but they must submit the full unredacted version to the Commission with reasons for the redaction
      - Language Gap Exposure. The breakdown of moderators by language forces public disclosure of any lack of native-language support in specific EU markets, making 'under-moderated' regions visible
      - Dual Clocks. The standard transparency stats (moderation actions) are on a 6-month cycle, while the systemic risk/audit accountability reports are generally on an annual cycle, meaning reports will contain different 'layers' of data depending on the time of year

  - article: 43
    title: Supervisory fee
    context: |
      (101) "The Commission should be in possession of all the necessary resources, in terms of staffing, expertise, and financial means, for the performance of its tasks under this Regulation. In order to ensure the availability of the resources necessary for the adequate supervision at Union level under this Regulation, and considering that Member States should be entitled to charge providers established in their territory a supervisory fee to in respect of the supervisory and enforcement tasks exercised by their authorities, the Commission should charge a supervisory fee, the level of which should be established on an annual basis, on very large online platforms and very large online search engines. The overall amount of the annual supervisory fee charged should be established on the basis of the overall amount of the costs incurred by the Commission to exercise its supervisory tasks under this Regulation, as reasonably estimated beforehand. Such amount should include costs relating to the exercise of the specific powers and tasks of supervision, investigation, enforcement and monitoring in respect of providers of very large online platforms and of very large online search engines, including costs related to the designation of very large online platforms and of very large online search engines or to the set up, maintenance and operation of the databases envisaged under this Regulation. It should also include costs relating to the set-up, maintenance and operation of the basic information and institutional infrastructure for the cooperation among Digital Services Coordinators, the Board and the Commission, taking into account the fact that in view of their size and reach very large online platforms and very large online search engines have a significant impact on the resources needed to support such infrastructure. The estimation of the overall costs should take into account the supervisory costs incurred in the previous year including, where applicable, those costs exceeding the individual annual supervisory fee charged in the previous year. The external assigned revenues resulting from the annual supervisory fee could be used to finance additional human resources, such as contractual agents and seconded national experts, and other expenditure related to the fulfilment of the tasks entrusted to the Commission by this Regulation. The annual supervisory fee to be charged on providers of very large online platforms and of very large online search engines should be proportionate to the size of the service as reflected by the number of its active recipients of the service in the Union. Moreover, the individual annual supervisory fee should not exceed an overall ceiling for each provider of very large online platforms or of very large online search engines taking into account the economic capacity of the provider of the designated service or services."
    key_requirements:
      - Provider must pay an annual supervisory fee to the Commission upon designation.
      - The fee contributes to the overall costs incurred by the Commission for its supervisory tasks under the DSA (including designation costs, database maintenance, and supporting the Board).
      - The individual fee amount is calculated based on the provider's share of active recipients in the Union relative to other designated providers.
      - The fee shall not exceed 0.05% of the provider's worldwide annual net income in the preceding financial year.
      - If a provider's fee is capped by the 0.05% ceiling, the residual amount is redistributed among other designated providers.
      - The tool must be careful not to confuse "turnover" (revenue) with "net income" (profit). If a VLOP operates at a loss (negative net income), the application of the 0.05% cap implies their fee might be zero or purely nominal, shifting the burden to profitable VLOPs. The Commentary notes this as a "potentially complicating factor".
      - The fee is charged per designated service. However, the 0.05% cap is calculated based on the economic capacity of the provider (the undertaking). If Amazon has two designated services (e.g., Store and Advertising), the fee is calculated for each, but the cap applies to Amazon's total net income.
      - The fee is not general tax revenue; it is "external assigned revenue". This means it is earmarked specifically for DSA enforcement costs (hiring experts, IT tools) and cannot be diverted to other EU budget lines.
